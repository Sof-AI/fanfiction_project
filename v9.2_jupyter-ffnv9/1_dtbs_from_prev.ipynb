{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5738ad9a",
   "metadata": {},
   "source": [
    "# Making Databases from Previous Versions' Data\n",
    "- <b>Name:</b> Sofia Kobayashi\n",
    "- <b>Date:</b> 01/02/2023\n",
    "- <b>Notebook Stage:</b> 1.0 (initial data collection & cleaning), finished\n",
    "- <b>Description:</b> Collecting all the data stored in all previous versions (v9.1 and below), and trying to collate all data, sort by work type & make into databases\n",
    "    - ONLY url data added here, much text data will be added in future notebooks\n",
    "\n",
    "### **<u>Table of Contents</u>**\n",
    "\n",
    "1. **[Imports](#imports)**\n",
    "1. **[Load in ALL urls](#load_urls)**\n",
    "    - All urls up until 01/13/2023\n",
    "1. **[Create 4 (sources) Database](#4_dtb)**\n",
    "    - v7_sheets, v8_old_local_files, pre (urls collected before 1/13/23), cur (urls collected on 1/13/23)\n",
    "    - cols: url, dtb_type, date_added, date_last_viewed, smk_source\n",
    "1. **[Combine 4 (sources) DTBs -> 1 big DTB (of all URLs)](#4_to_1_dtb)**\n",
    "1. **[Adding/filling metadata cols](#m1)**\n",
    "    - filled date_added, added version num\n",
    "1. **[Seperate AO3 works vs Others](#seperate_others)**\n",
    "    - **Others CHECKPOINT 0** - all non-AO3 urls & AO3 external works\n",
    "1. **[Label AO3 URLs & Separate into (work) DTBs](#seperate_ao3_dtbs)**\n",
    "1. **[Clean up & customize the new 7 databases](#clean_7)**\n",
    "    - 7 DTBs: collections, comments, search, series, tags, users, works\n",
    "    - **Col, Comments, Search, Tags, Users CHECKPOINTS 0** - in 'data-checkpoints' folder\n",
    "1. **[Fill seriesDTB](#fill_series)**\n",
    "    - **Series CHECKPOINT 1** - all series urls (as of 1/13/23) w/ basic meta data, de-dupped\n",
    "        - did not add text-data series\n",
    "1. **[Clean & prepare ficDTB](#fill_fics)**\n",
    "    - [**Fic CHECKPOINT 1**](#fill_fics_1) - *all* fic urls w/ basic metadata\n",
    "        - Cols: dtb_type, smk_source, version, date_added, date_last_viewed, url_type, id, url, col_work\n",
    "    - [**Fic CHECKPOINT 2**](#fill_fics_2) - fic urls fic-de-dupped (no fic info)\n",
    "        - Added cols: url_psueds, cur_chapter, notes\n",
    "    - [**Fic CHECKPOINT 3**](#fill_fics_3) - reorganized cols, added more, prepared for adding fic info (no fic info)\n",
    "        - Added cols: dtb_type, smk_source, version\tdate_added\tdate_last_viewed\turl_type\tid\turl\tcol_work\turl_psueds\tcur_chapter\tnotes\ttitle\tauthors\tfandoms\tfic_obj\tdate_obj_updated\n",
    "        - PLUS all other metadata that could be added by AO3 API\n",
    "        - did not all text-data fics, only url-data fics--text data fics to be added in next notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd1de3",
   "metadata": {},
   "source": [
    "<a id=\"imports\"></a>\n",
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908ed844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "%run helpers.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "cur_date = datetime.now().strftime('%m-%d-%y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea591bd4",
   "metadata": {},
   "source": [
    "<a id=\"load_urls\"></a>\n",
    "\n",
    "## 2. Load in ALL urls (up until 01/13/23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all urls from v7\n",
    "with open(\"urlsOutput/v7_json/v7_authors.json\", \"r\") as infile:\n",
    "    v7_authors = json.load(infile) \n",
    "    \n",
    "with open(\"urlsOutput/v7_json/v7_lookInto.json\", \"r\") as infile:\n",
    "    v7_lookInto = json.load(infile)  \n",
    "    \n",
    "with open(\"urlsOutput/v7_json/v7_others.json\", \"r\") as infile:\n",
    "    v7_others = json.load(infile) \n",
    "    \n",
    "with open(\"urlsOutput/v7_json/v7_readUrls.json\", \"r\") as infile:\n",
    "    v7_read = json.load(infile) \n",
    "    \n",
    "with open(\"urlsOutput/v7_json/v7_toReadUrls.json\", \"r\") as infile:\n",
    "    v7_toRead = json.load(infile) \n",
    "    \n",
    "with open(\"urlsOutput/v7_json/v7_unsortedUrls.json\", \"r\") as infile:\n",
    "    v7_unsorted = json.load(infile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d12b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"urlsOutput/chrome_1.json\", \"r\") as infile:\n",
    "    chrome = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all urls collected BEFORE 1/13/23\n",
    "# with open(\"urlsOutput/oldUrls_01-13-23.json\", \"r\") as infile:\n",
    "#     pre = json.load(infile) \n",
    "#     pre = pd.DataFrame(pre).drop_duplicates(subset=[0])[0].to_list() # de-dup pre list\n",
    "\n",
    "with open(\"urlsOutput/pre.json\", \"r\") as infile:\n",
    "    pre = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all urls collected on 1/13/23\n",
    "with open(\"urlsOutput/readinglist_01-13-23.json\", \"r\") as infile:\n",
    "    cur = json.load(infile) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbae557",
   "metadata": {},
   "source": [
    "<a id=\"4_dtb\"></a>\n",
    "## 3. Create 4 databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18aad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDatetime(x):\n",
    "    if x is None: return x\n",
    "    else: return datetime.strptime(x, '%m-%d-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be022fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCur = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f4baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cur df - dates added, filter SECOND\n",
    "from datetime import datetime\n",
    "\n",
    "# load in cur\n",
    "dfCur = pd.DataFrame(cur)\n",
    "\n",
    "# convert dates\n",
    "dfCur[\"date_added\"] = dfCur[\"dateAdded\"].apply(toDatetime)\n",
    "dfCur[\"date_last_viewed\"] = dfCur[\"dateLastViewed\"].apply(toDatetime)\n",
    "dfCur[\"smk_source\"] = \"safari\"\n",
    "dfCur[\"dtb_type\"] = np.nan\n",
    "\n",
    "# sort by date added\n",
    "dfCur = dfCur.sort_values(by=[\"date_added\"]).drop(columns=[\"dateAdded\",\"dateLastViewed\"])\n",
    "\n",
    "# drop url duplicates (keep url added first)\n",
    "dfCur = dfCur.drop_duplicates(subset=[\"url\"], keep='first')\n",
    "\n",
    "dfCur.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b5b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre df\n",
    "dfPre = pd.DataFrame(pre).rename(columns={0:\"url\"})\n",
    "dfPre[\"date_added\"] = np.datetime64(\"NaT\")\n",
    "dfPre[\"date_last_viewed\"] = np.datetime64(\"NaT\")\n",
    "dfPre[\"dtb_type\"] = np.nan\n",
    "dfPre[\"smk_source\"] = \"v8_local_url_files\"\n",
    "\n",
    "dfPre = dfPre.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "dfPre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8478d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicDF(myList, dtb_type, dateAdded=np.datetime64(\"NaT\"), dateLastViewed=np.datetime64(\"NaT\"), source=\"v7_sheets\"):\n",
    "    \"\"\"\n",
    "    Takes a list -> DF, adds dtb_type, date_added & date_last_viewed, de-dups\n",
    "    \"\"\"\n",
    "    dfTemp = pd.DataFrame(myList).rename(columns={0:\"url\"}) # read in list\n",
    "    \n",
    "    # add cols\n",
    "    dfTemp[\"dtb_type\"] = dtb_type\n",
    "    dfTemp[\"date_added\"] = dateAdded\n",
    "    dfTemp[\"date_last_viewed\"] = dateLastViewed\n",
    "    dfTemp[\"smk_source\"] = source\n",
    "    \n",
    "    # de-dup\n",
    "    dfTemp = dfTemp.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "    return dfTemp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd81e1",
   "metadata": {},
   "source": [
    " - 01/01/2022 is default date for all v7 stuff\n",
    " \n",
    " - priortize v7 if date is NOT 01-01-22 00:00:01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6385fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines v7 df - filter FIRST, date_added = ~v8 date\n",
    "date1 = datetime.strptime(\"01-01-22 00:00:01\", '%m-%d-%y %H:%M:%S')\n",
    "\n",
    "df_authors = basicDF(v7_authors, \"author\", date1)\n",
    "df_lookInto = basicDF(v7_lookInto, \"look_into\", date1)\n",
    "df_read = basicDF(v7_read, \"read\", date1)\n",
    "df_toRead = basicDF(v7_toRead, \"to_read\", date1)\n",
    "df_unsorted = basicDF(v7_unsorted, np.nan, date1)\n",
    "\n",
    "# combines all v7 dfs\n",
    "v7_df = pd.concat([df_authors, df_read, df_toRead, df_lookInto, df_unsorted]) # in order of priority, first kept\n",
    "v7_df = v7_df.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "v7_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a48ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chrome df - no dates\n",
    "dfChrome = pd.DataFrame(chrome).rename(columns={0:\"url\"})\n",
    "dfChrome[\"date_added\"] = np.datetime64(\"NaT\")\n",
    "dfChrome[\"date_last_viewed\"] = np.datetime64(\"NaT\")\n",
    "dfChrome[\"dtb_type\"] = np.nan\n",
    "dfChrome[\"smk_source\"] = \"chrome\"\n",
    "\n",
    "dfChrome = dfChrome.drop_duplicates(subset=[\"url\"])\n",
    "\n",
    "dfChrome.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa55743",
   "metadata": {},
   "source": [
    "### DATABASE\n",
    "1. url - url\n",
    "1. date_added - closet guess to when it was added, either from safari reading list data or version guess\n",
    "    - v9 - 10-24-2022 00:00:01\n",
    "    - v8 - 06-04-2022 00:00:01\n",
    "    - v7 - 05/01/2021 00:00:01\n",
    "1. date_last_viewed - date\n",
    "    - only from safari reading list data\n",
    "1. dtb_type - dtb type\n",
    "    - read, toRead, lookInto, other, authors, NaN (unsorted)\n",
    "1. smk_source - where I found url/where v9.2 is pulling it from\n",
    "    - v8_local_url_files\n",
    "    - v7_sheets\n",
    "    - safari\n",
    "    - chrome\n",
    "    - txt_fic (not added yet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c6055",
   "metadata": {},
   "source": [
    "<a id=\"4_to_1_dtb\"></a>\n",
    "## 4. Combine all 4 -> 1 (not added v1-6 dates)\n",
    "- order: dfPre, dfChrome, dfCur, v7_df\n",
    "    - dfPre + dfChrome, update (only urls will be concat'd)\n",
    "    - +dfCur (add urls, overwrite with dateAdded & dateLastViewed)\n",
    "    - rename v7_df.date_added -> v7_date_added\n",
    "    - rename total.date_added -> cur_date_added\n",
    "    - +v7_df (add urls, overwrite dtb_type, leaves dateLastViewed alone)\n",
    "    - compare v7_date_added vs cur_date_added -> keep earliest one\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8876883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfPre + dfChrome\n",
    "total = new_combine(dfPre, dfChrome)\n",
    "total.smk_source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d04b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# +dfCur\n",
    "total = new_combine(total, dfCur)\n",
    "total.smk_source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cc7a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename v7 & total's date_added\n",
    "total = total.rename(columns={\"date_added\": \"cur_date_added\"})\n",
    "v7_df = v7_df.rename(columns={\"date_added\": \"v7_date_added\"})\n",
    "total = new_combine(total, v7_df)\n",
    "total.smk_source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a734452e",
   "metadata": {},
   "source": [
    "<a id='m1'></a>\n",
    "## 5. Combined!  Now add/fill metadata cols:\n",
    "- fill date_added\n",
    "- add version nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de8c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create total.date_added from the earliest date from 'cur_date_added' vs 'v7_date_added'\n",
    "total[\"date_added\"] = np.datetime64(\"NaT\")\n",
    "\n",
    "for ind in total.index:\n",
    "    v7 = total.at[ind, \"v7_date_added\"]\n",
    "    cur = total.at[ind, \"cur_date_added\"]\n",
    "    newDate = np.datetime64(\"NaT\")\n",
    "    if pd.isnull(v7) and pd.isnull(cur): newDate= np.datetime64(\"NaT\")\n",
    "    elif not pd.isnull(v7) and not pd.isnull(cur): newDate= min([v7,cur])\n",
    "    elif not pd.isnull(v7): newDate= v7\n",
    "    else: newDate= cur\n",
    "    \n",
    "    total.at[ind, \"date_added\"] = newDate\n",
    "\n",
    "total_2 = total.drop(columns=[\"v7_date_added\",\"cur_date_added\"])\n",
    "total_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baff66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all version nums (based on smk_source) (not added v1-6 or text fics)\n",
    "total_2[\"version\"] = 7\n",
    "\n",
    "for ind in total_2.index:\n",
    "    source = total_2.at[ind, \"smk_source\"]\n",
    "    verNum = 7\n",
    "    \n",
    "    if source == \"v8_local_url_files\": verNum = 8\n",
    "    elif source == \"safari\" or source == \"chrome\": verNum = 9\n",
    "    \n",
    "    total_2.at[ind, \"version\"] = verNum\n",
    "    \n",
    "total_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9145bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add date_added (by version number) for all non-date entities\n",
    "# v9 - 10-24-2022 00:00:01\n",
    "# v8 - 06-04-2022 00:00:01\n",
    "# v7 - 05/01/2021 00:00:01\n",
    "\n",
    "v9_date = datetime.strptime(\"10-24-22 00:00:01\", '%m-%d-%y %H:%M:%S')\n",
    "v8_date = datetime.strptime(\"06-04-22 00:00:01\", '%m-%d-%y %H:%M:%S')\n",
    "v7_date = datetime.strptime(\"05-01-21 00:00:01\", '%m-%d-%y %H:%M:%S')\n",
    "\n",
    "for ind in total_2.index:\n",
    "    added = total_2.at[ind, \"date_added\"]\n",
    "    if pd.isnull(added):\n",
    "        new_date = 0\n",
    "        source = total_2.at[ind, \"smk_source\"]\n",
    "        if source == \"v7_sheets\": new_date = v7_date\n",
    "        elif source == \"v8_local_url_files\": new_date = v8_date\n",
    "        elif source == \"safari\" or source == \"chrome\": new_date = v9_date\n",
    "        total_2.at[ind, \"date_added\"] = new_date\n",
    "    \n",
    "total_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24906680",
   "metadata": {},
   "source": [
    "<a id=\"seperate_others\"></a>\n",
    "## 6. Seperate AO3 works vs Others (non-ao3 & ao3 external works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9a2588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get others DF (non-AO3 and ao3 external works)\n",
    "external = total_2.query(\"url.str.contains('archiveofourown.org/external_works/')\")\n",
    "dfOther = total_2.query(\"~url.str.contains('archiveofourown.org/')\")\n",
    "dfOther = pd.concat([external, dfOther]).reset_index(drop=True)\n",
    "\n",
    "dfOther.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4161380b",
   "metadata": {},
   "source": [
    "#### Others CHECKPOINT! others-0-all.csv (all non-AO3 urls & AO3 external works from all urls post 1/13/23, inclusive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfOther.to_csv(\"data-checkpoints/others-0-all_01-03-23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0798d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get non-other (aka all ao3) dtb\n",
    "total_3 = total_2.query(\"url.str.contains('archiveofourown.org/')\") \\\n",
    "        .query(\"~url.str.contains('archiveofourown.org/external_works/')\") \\\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "total_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d79e3b",
   "metadata": {},
   "source": [
    "<a id='seperate_ao3_dtbs'></a>\n",
    "## 7. Label ao3 work_type & separate into DTBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e422523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add new columns for id & url_type\n",
    "total_3[\"id\"] = np.nan\n",
    "total_3[\"url_type\"] = np.nan\n",
    "\n",
    "# label all cols\n",
    "for ind in total_3.index:\n",
    "    url = total_3.at[ind, \"url\"]\n",
    "    \n",
    "    data = getTypeAndId(url)\n",
    "    wType = data[0]\n",
    "    wId = data[1]\n",
    "    \n",
    "    total_3.at[ind, \"url_type\"] = wType\n",
    "    total_3.at[ind, \"id\"] = wId\n",
    "\n",
    "total_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorganize cols order\n",
    "total_3[\"location_found\"] = \"AO3\"\n",
    "all_ao3_links = total_3[[\"dtb_type\",\"location_found\",\"smk_source\",\"version\",\n",
    "                   \"date_added\",\"date_last_viewed\",\"url_type\",\"id\",\"url\"]]\n",
    "# all_ao3_links.to_csv(f\"data-checkpoints/all_ao3_links_until_01-13-23__{cur_date}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4962ac8c",
   "metadata": {},
   "source": [
    "#### ** All AO3 links including 1-13-23, inclusive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f098b",
   "metadata": {},
   "source": [
    "<a id='clean_7'></a>\n",
    "## 8. Clean up & customize the new 7 databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777958e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ao3_links = pd.read_csv(\"data-checkpoints/all_ao3_links_until_01-13-23__02-26-23.csv\", \n",
    "                            index_col=0,\n",
    "                            parse_dates = ['date_added','date_last_viewed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0246ab",
   "metadata": {},
   "source": [
    "### 8.1) Clean & save Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7735f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate collections from all_ao3_links\n",
    "colDTB = all_ao3_links.query(\"url_type == 'collections'\") \\\n",
    "                .reset_index(drop=True).rename(columns={\"id\":\"name\"}) \\\n",
    "                .drop(columns=[\"url_type\"])\n",
    "\n",
    "# correct data input mistake\n",
    "colDTB.at[0,\"name\"] = \"Canon_Divergence\"\n",
    "\n",
    "# write first col checkpoint\n",
    "colDTB.to_csv(f\"data-checkpoints/col-0-all_{cur_date}.csv\")\n",
    "\n",
    "colDTB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12e6fe",
   "metadata": {},
   "source": [
    "#### ** Collections CHECKPOINT! - \"col-0-all_02-26-23.csv\" (all collections until this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce295be",
   "metadata": {},
   "source": [
    "### 8.2) Clean & save Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a74636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning searches\n",
    "searchDTB = all_ao3_links.query(\"url_type == 'search'\") \\\n",
    "                .reset_index(drop=True).rename(columns={\"id\":\"search_str\"}) \\\n",
    "                .drop(columns=[\"url_type\"])\n",
    "\n",
    "# write first col checkpoint\n",
    "searchDTB.to_csv(f\"data-checkpoints/search-0-all_{cur_date}.csv\")\n",
    "\n",
    "searchDTB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3618e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning series\n",
    "seriesDTB = all_ao3_links.query(\"url_type == 'series'\") \\\n",
    "                .reset_index(drop=True) \\\n",
    "                .drop(columns=[\"url_type\"])\n",
    "\n",
    "# write first col checkpoint\n",
    "seriesDTB.to_csv(f\"data-checkpoints/series-0-all_{cur_date}.csv\")\n",
    "\n",
    "seriesDTB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a76e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning tags\n",
    "tagsDTB = all_ao3_links.query(\"url_type == 'tags'\") \\\n",
    "                .reset_index(drop=True) \\\n",
    "                .rename(columns={\"id\":\"tag_str\"}) \\\n",
    "                .drop(columns=[\"url_type\"])\n",
    "\n",
    "tagsDTB[\"tag_type\"] = np.nan\n",
    "tagsDTB[\"tag_type\"] = tagsDTB[\"tag_type\"].astype(str)\n",
    "\n",
    "# write first col checkpoint\n",
    "tagsDTB.to_csv(f\"data-checkpoints/tags-0-all_{cur_date}.csv\")\n",
    "\n",
    "tagsDTB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning users\n",
    "usersDTB = all_ao3_links.query(\"url_type == 'users'\") \\\n",
    "                .reset_index(drop=True) \\\n",
    "                .rename(columns={\"id\":\"user_name\"}) \\\n",
    "                .drop(columns=[\"url_type\"])\n",
    "\n",
    "# write first col checkpoint\n",
    "usersDTB.to_csv(f\"data-checkpoints/users-0-all_{cur_date}.csv\")\n",
    "\n",
    "usersDTB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2dd101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cleaning fics\n",
    "ficDTB = all_ao3_links.query(\"url_type == 'chapters' or url_type == 'works' or \\\n",
    "                url_type.str.contains('collections:')\") \\\n",
    "                .reset_index(drop=True) \n",
    "\n",
    "# write first checkpoint\n",
    "ficDTB.to_csv(f\"data-checkpoints/fic-0-all_{cur_date}.csv\")\n",
    "\n",
    "ficDTB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e7afa",
   "metadata": {},
   "source": [
    "#### CHECKPOINT! - 0 (for all: collections, comments, search, tags, users)\n",
    "- stored in data-checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d1257",
   "metadata": {},
   "source": [
    "<a id='fill_series'></a>\n",
    "## 9. Fill seriesDTB (already de-dupped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "334b29ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_columns = {'dtb_type', 'location_found', 'smk_source', 'version', 'date_added',\n",
    "       'date_last_viewed', 'id', 'url', 'name', 'creators', 'fandoms',\n",
    "       'series_obj', 'date_obj_updated', 'description', 'notes', 'words',\n",
    "       'complete', 'is_subscribed', 'series_begun', 'series_updated',\n",
    "       'nbookmarks', 'nworks', 'work_list', 'is_restricted', 'not_found'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a8c26",
   "metadata": {},
   "source": [
    "<a id='fill_series.1'></a>\n",
    "### 9.1 Define Series-filling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "e827313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AO3.utils.limit_requests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "107d787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSeriesFandoms(series_obj):\n",
    "    \"\"\"\n",
    "    Takes an AO3.Series object.\n",
    "    Returns a list of all fandoms from all works in given series.\n",
    "    \"\"\"\n",
    "    fandoms = []\n",
    "    work_list = get_series_work_list(series_obj)\n",
    "    for work in work_list:\n",
    "        for fandom in work.fandoms:\n",
    "            if fandom not in fandoms:\n",
    "                fandoms.append(fandom)\n",
    "    \n",
    "    return fandoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee7c6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_series_work_list(series_obj):\n",
    "    try: return [work for work in series_obj.work_list]\n",
    "    except UnboundLocalError: return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "ea54d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_row_complete(row):\n",
    "    new_row = row.copy().drop(columns=['dtb_type','date_last_viewed', \n",
    "                                       'description', 'notes'])\n",
    "    return len(np.where(pd.isnull(new_row))[1]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "432888be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_series_ao3_info(series_id, session, report=False):\n",
    "    \"\"\"\n",
    "    Takes a series id (int).\n",
    "    Returns a 1-row pandas DF populated by ao3 data from the given series id.\n",
    "    \"\"\"\n",
    "    # initialize temp holder & Series obj\n",
    "    single_series = pd.DataFrame({'id': [series_id]})\n",
    "    series = AO3.Series(series_id, session=session)\n",
    "\n",
    "    # write report info\n",
    "    name = series.name\n",
    "    creators = json.dumps([user.username for user in series.creators])\n",
    "    fandoms = json.dumps(getSeriesFandoms(series))\n",
    "\n",
    "    single_series['name'] = name\n",
    "    single_series['creators'] = creators\n",
    "    single_series['fandoms'] = fandoms\n",
    "    if report: print(f\"- Wrote '{name}' by {creators}\\nin {fandoms}\")\n",
    "\n",
    "    # write remaining info\n",
    "    single_series['series_obj'] = series\n",
    "    single_series['date_obj_updated'] = datetime.now()\n",
    "    \n",
    "    single_series['description'] = series.description\n",
    "    single_series['notes'] = series.notes\n",
    "    single_series['words'] = series.words\n",
    "    single_series['complete'] = series.complete\n",
    "    single_series['is_subscribed'] = series.is_subscribed\n",
    "    \n",
    "    single_series['series_begun'] = series.series_begun\n",
    "    single_series['series_updated'] = series.series_updated\n",
    "    single_series['nbookmarks'] = series.nbookmarks\n",
    "    single_series['nworks'] = series.nworks\n",
    "    single_series['work_list'] = json.dumps([work.id for work in get_series_work_list(series)])\n",
    "    \n",
    "    single_series['is_restricted'] = series._soup.find(\"img\", {\"title\": \"Restricted\"}) is not None\n",
    "    single_series['not_found'] = False\n",
    "    \n",
    "    return single_series\n",
    "\n",
    "# fill_series_ao3_info(1575793, my_session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "2fe529bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_series_dtb(initial_series_dtb, session, update=False, report=False):\n",
    "    \"\"\"\n",
    "    Takes a seriesDTB (pandas DataFrame, post series-0), an AO3 session, a Boolean to put in 'update' mode \n",
    "        (aka update all rows regardless of it they're already complete) and a Boolean to print report.\n",
    "    Modifies given seriesDTB by filling it up with ao3 information.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    # find total number of series to fill\n",
    "    total = max(initial_series_dtb.index)\n",
    "    \n",
    "    # ensure initial_series_dtb has all necessary columns\n",
    "    for col in series_columns:\n",
    "        if col not in initial_series_dtb.columns:\n",
    "            initial_series_dtb[col] = np.nan\n",
    "\n",
    "    # fill all series/rows in initial_series_dtb\n",
    "    for ind in initial_series_dtb.index: \n",
    "        try: \n",
    "            # when not using full-report, alert at every 100 series\n",
    "            if not report:\n",
    "                if ind%100 == 0: \n",
    "                    print(f'- {ind}! (printed every 100)')\n",
    "            \n",
    "            # if series/row not entirely filled in OR we're updating the dtb \n",
    "            if (not series_row_complete(initial_series_dtb.iloc[[ind]])) or update: \n",
    "                # get series id\n",
    "                series_id = initial_series_dtb.at[ind, \"id\"]\n",
    "                if report: print(f\"{ind}: [{(ind/total)*100:.2f}%] Filling for [{series_id}]\")\n",
    "                \n",
    "                # get ao3 info\n",
    "                series_ao3_info = fill_series_ao3_info(series_id, session, report=report)\n",
    "                \n",
    "                # update initial_series_dtb with series_ao3_info (new info will overwrite old info)\n",
    "                series_ao3_info.index = [ind]\n",
    "                initial_series_dtb.update(series_ao3_info, join='left', overwrite=True)\n",
    "            \n",
    "            # if series/row is satifactory\n",
    "            else: \n",
    "                if report: print(f\"{ind}: .\")\n",
    "        \n",
    "        # if something goes wrong \n",
    "        except Exception as e:\n",
    "            initial_series_dtb.at[ind, \"not_found\"] = True\n",
    "            print(f\"-- ERROR, {ind}: {initial_series_dtb.at[ind, 'id']}\")\n",
    "        \n",
    "        # update temp csv w/ new row/series\n",
    "        initial_series_dtb.to_csv(\"temp_series.csv\")\n",
    "\n",
    "    # Write finished series DTB to csv\n",
    "    initial_series_dtb.to_csv(\"temp_series_final.csv\")\n",
    "\n",
    "    print(\"\\nDONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0590ed52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "db36e6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3d638c8",
   "metadata": {},
   "source": [
    "<a id='fill_series.2'></a>\n",
    "### 9.2 Fill SeriesDTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "186202ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most recent series data\n",
    "seriesDTB = pd.read_csv(\"data-checkpoints/series-0-all_02-26-23.csv\", \n",
    "                        index_col=0,\n",
    "                        parse_dates=['date_added', 'date_last_viewed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f7fea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inititalize session\n",
    "ss1 = my_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "dece760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill_series_dtb(seriesDTB, ss1, update=False, report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f34e24",
   "metadata": {},
   "source": [
    "#### Series CHECKPOINT! (series-1, all & clean) (saved csv files in data-checkpoints)\n",
    "- most recent: 03-06-23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63a837",
   "metadata": {},
   "source": [
    "<a id='fill_fics'></a>\n",
    "## 10. Clean & prepare ficDTB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee893f7",
   "metadata": {},
   "source": [
    "<a id='fill_fics_1'></a>\n",
    "### 10.1 Clean prev data in ficDTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add empty info cols\n",
    "if False: \n",
    "    ficDTB[\"title\"] = np.nan\n",
    "    ficDTB[\"authors\"] = np.nan\n",
    "    ficDTB[\"fandoms\"] = np.nan\n",
    "    \n",
    "    ficDTB[\"fic_obj\"] = np.nan\n",
    "    ficDTB[\"date_obj_updated\"] = np.nan\n",
    "    ficDTB[\"date_obj_updated\"] = pd.to_datetime(ficDTB[\"date_obj_updated\"])\n",
    "    \n",
    "    ficDTB[\"url_pseuds\"] = np.nan\n",
    "    ficDTB[\"col_work\"] = np.nan\n",
    "\n",
    "ficDTB.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bf3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add col_work collections\n",
    "def getCol(x):\n",
    "    if not \"collections:\" in x: return np.nan\n",
    "    else:\n",
    "        col = x.replace(\"collections:\",\"\")\n",
    "        return col\n",
    "\n",
    "# ficDTB[\"col_work\"] = ficDTB[\"url_type\"].apply(getCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update url_types (aka all 'collections:' -> col_work)\n",
    "def x(x):\n",
    "    if not \"collections:\" in x: return x\n",
    "    else: return \"col_work\"\n",
    "\n",
    "# ficDTB[\"url_type\"] = ficDTB[\"url_type\"].apply(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb142e1",
   "metadata": {},
   "source": [
    "#### Fic CHECKPOINT! (fic-1, all & clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ca8dd",
   "metadata": {},
   "source": [
    "<a id='fill_fics_2'></a>\n",
    "### 10.2 De-dup ficDTB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93d36f",
   "metadata": {},
   "source": [
    "#### ficDTB Columns\n",
    "- dtb_type: (read, to_read/NaN)\n",
    "    - read\n",
    "    - to_read\n",
    "    - NaN\n",
    "- smk_source: (v7, v8, safari, chrome)\n",
    "    - v7_sheets\n",
    "    - v8_local_url_files\n",
    "    - chrome\n",
    "    - safari\n",
    "- version: 7-9 (earliest)\n",
    "- date_added: (earliest)\n",
    "    - date\n",
    "    - NaT\n",
    "- date_last_viewed (latest)\n",
    "    - date\n",
    "    - NaT\n",
    "- url_type (works) (add others to url_psueds)\n",
    "    - works\n",
    "    - col_work\n",
    "    - chapters\n",
    "- id: num\n",
    "- url: url\n",
    "- col_work: list of col names\n",
    "- url_psueds: list of non-main urls\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f04f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in ficDTB from checkpoint\n",
    "ficDups = pd.read_csv(\"data-checkpoints/fic-1-clean_01-17-23_11-42-48.csv\", index_col=0, \n",
    "                      parse_dates=[\"date_added\",\"date_last_viewed\"])\n",
    "ficDups[\"url_psueds\"] = json.dumps([])\n",
    "ficDups.head() # fic_obj col no longer holds functional Fic objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5212667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectInfo(dtbCol, orderList):\n",
    "    infos = wSlice[dtbCol].to_list()\n",
    "    temp_info = np.nan\n",
    "    for info in orderList:\n",
    "        if info in infos: \n",
    "            temp_info = info\n",
    "            break\n",
    "            \n",
    "    return temp_info\n",
    "\n",
    "# all_smk_sources = [\"v7_sheets\",\"v8_local_url_files\",\"safari\",\"chrome\"]\n",
    "# selectInfo(\"smk_source\", all_smk_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea4f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyFicDTB():\n",
    "    # initialize res\n",
    "    res = pd.DataFrame(columns=['dtb_type',\n",
    "                                 'smk_source',\n",
    "                                 'version',\n",
    "                                 'date_added',\n",
    "                                 'date_last_viewed',\n",
    "                                 'url_type',\n",
    "                                 'id',\n",
    "                                 'url',\n",
    "                                 'col_work',\n",
    "                                 'url_psueds'])\n",
    "    \n",
    "    # make right dtypes\n",
    "    res[\"version\"] = res[\"version\"].astype(\"int\")\n",
    "    res[\"id\"] = res[\"id\"].astype(\"int\")\n",
    "    \n",
    "    res[\"date_added\"] = pd.to_datetime(res[\"date_added\"])\n",
    "    res[\"date_last_viewed\"] = pd.to_datetime(res[\"date_last_viewed\"])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e6b77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mergeFics(ficId, ficDups):\n",
    "    \"\"\"\n",
    "    Takes AO3 fic id, finds all fics with the same id in ficDups, merges them into one row.\n",
    "    May need to update row all-value lists\n",
    "    Returns that single row.\n",
    "    \"\"\"\n",
    "    # get rows with same fic id\n",
    "    wSlice = ficDups[ficDups[\"id\"] == ficId].copy()\n",
    "    # print(wSlice)\n",
    "    \n",
    "    if len(temp) > 1:\n",
    "        # initialize temp result DF\n",
    "        res = emptyFicDTB()\n",
    "\n",
    "        # write dtb_type\n",
    "        res.at[0,\"dtb_type\"] = selectInfo(\"dtb_type\", all_dtb_types)\n",
    "\n",
    "        # write smk_source\n",
    "        res.at[0,\"smk_source\"] = selectInfo(\"smk_source\", all_smk_sources)\n",
    "\n",
    "        # write version\n",
    "        res.at[0,\"version\"] = min(wSlice[\"version\"].to_list())\n",
    "\n",
    "        # write date_added\n",
    "        res.at[0,\"date_added\"] = min(wSlice[\"date_added\"].to_list())\n",
    "\n",
    "        # write date_last_viewed\n",
    "        res.at[0,\"date_last_viewed\"] = max(wSlice[\"date_last_viewed\"].to_list())\n",
    "\n",
    "        # write id\n",
    "        res.at[0,\"id\"] = ficId\n",
    "\n",
    "        # write col_work\n",
    "        res.at[0,\"col_work\"] = json.dumps([x for x in wSlice[\"col_work\"].to_list() if not pd.isnull(x)])\n",
    "\n",
    "        # write url\n",
    "        url = wSlice.iloc[0][\"url\"]\n",
    "        res.at[0,\"url\"] = url\n",
    "\n",
    "        # write url_type\n",
    "        wType = getTypeAndId(url)[0]\n",
    "        if \"collections:\" in wType: wType = \"col_work\"\n",
    "        res.at[0,\"url_type\"] = wType\n",
    "        \n",
    "        # write url_psueds\n",
    "        psueds = wSlice.url.drop_duplicates().to_list()[1:]\n",
    "        if psueds is None: psueds = []\n",
    "        res.at[0,\"url_psueds\"] = json.dumps(psueds)\n",
    "        \n",
    "    else:\n",
    "        res = wSlice.iloc[0]\n",
    "    \n",
    "    return res\n",
    "        \n",
    "# mergeFics(19413088, ficDups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c02a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deDupFics(ficDTB):\n",
    "    \"\"\"\n",
    "    Takes a ficDTB, de-dups all fics in a way that preserves desired fic info order. Prints a progress bar.\n",
    "    Returns a ficDTB with no duplicate fics & data preserved.\n",
    "    \"\"\"\n",
    "    # fic info order (for dtb_types & sml_sources)\n",
    "    all_dtb_types = [\"read\",\"to_read\"]\n",
    "    all_smk_sources = [\"v7_sheets\",\"v8_local_url_files\",\"safari\",\"chrome\"]\n",
    "    \n",
    "    # make temp holder & get all ids\n",
    "    temp_ficDTB = emptyFicDTB()\n",
    "    ids = ficDTB.id.drop_duplicates().to_list()\n",
    "    total = len(ids)\n",
    "    \n",
    "    # print report\n",
    "    print(f\"TOTAL: {total} ids to de-dup! {(total//100+1)*'|'}\")\n",
    "    print(f\"{len(str(total))*' '}              PROGESS: \",end='')\n",
    "    \n",
    "    # add newly merged fics\n",
    "    for i, wId in enumerate(ids):\n",
    "        temp_row = mergeFics(wId, ficDTB)\n",
    "        temp_ficDTB = pd.concat([temp_ficDTB, temp_row])\n",
    "        \n",
    "        # progress bar\n",
    "        if (i%100) == 0: print('|', end=\"\")\n",
    "\n",
    "    return temp_ficDTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b0258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-dup ficDTB\n",
    "ficDups = pd.read_csv(\"data-checkpoints/fic-1-clean_01-17-23_11-42-48.csv\", index_col=0, \n",
    "                      parse_dates=[\"date_added\",\"date_last_viewed\"])\n",
    "ficDups[\"version\"] = ficDups[\"version\"].astype(\"int\")\n",
    "ficDups[\"id\"] = ficDups[\"id\"].astype(\"int\")\n",
    "\n",
    "ficNoDups = deDupFics(ficDups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write checkpoint\n",
    "ficNoDups[\"version\"] = ficNoDups[\"version\"].astype(\"int\")\n",
    "ficNoDups[\"id\"] = ficNoDups[\"id\"].astype(\"int\")\n",
    "\n",
    "# ficNoDups.to_csv(\"data-checkpoints/fic-2-all_01-18-23.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b49e94",
   "metadata": {},
   "source": [
    "#### Fic CHECKPOINT! (fic-2, all & fic de-dupped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb492e08",
   "metadata": {},
   "source": [
    "<a id='fill_fics_3'></a>\n",
    "### 10.3 Prepare ficDTB to be filled by AO3 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe686803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in checkpoint\n",
    "ficDTB = pd.read_csv(\"data-checkpoints/fic-2-all_01-18-23.csv\", index_col=0,\n",
    "                     parse_dates=[\"date_added\",\"date_last_viewed\"]) \\\n",
    "                    .reset_index(drop=True) \\\n",
    "                    .drop(columns=['cur_chapter','notes'])\n",
    "ficDTB.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a822d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all col_name groups\n",
    "cols = ['location_found','is_missing',\n",
    "        'dtb_type','smk_source','version','date_added','date_last_viewed','url_type','id','url',\n",
    "        'recced_from_collections','url_psueds']\n",
    "\n",
    "tags = ['title','authors','fandoms','rating','categories','warnings',\n",
    "        'relationships','characters','tags','series','collections']\n",
    "\n",
    "meta = ['words','nchapters','expected_chapters', 'complete',\n",
    "        'date_published','date_updated','date_edited',\n",
    "        'language','restricted','metadata',]\n",
    "\n",
    "text = ['summary','start_notes','end_notes','chapters','text',]\n",
    "\n",
    "stats = ['kudos','comments','bookmarks','hits',]\n",
    "\n",
    "\n",
    "tags.extend(meta)\n",
    "tags.extend(text)\n",
    "tags.extend(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc00fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cols to be used\n",
    "if True: \n",
    "    for col in tags:\n",
    "        ficDTB[col] = np.nan\n",
    "        \n",
    "    man_tags = ['fic_obj','date_obj_updated','is_subscribed','cur_chapter','notes']\n",
    "    for tag in man_tags:\n",
    "        ficDTB[tag] = np.nan\n",
    "        \n",
    "    ficDTB[\"location_found\"] = 'AO3'\n",
    "    ficDTB[\"is_missing\"] = np.nan\n",
    "    \n",
    "    ficDTB = ficDTB.rename(columns={\"col_work\":\"recced_from_collections\"})\n",
    "\n",
    "    ficDTB[\"date_obj_updated\"] = pd.to_datetime(ficDTB[\"date_obj_updated\"])\n",
    "    ficDTB[\"date_published\"] = pd.to_datetime(ficDTB[\"date_published\"])\n",
    "    ficDTB[\"date_updated\"] = pd.to_datetime(ficDTB[\"date_updated\"])\n",
    "    ficDTB[\"date_edited\"] = pd.to_datetime(ficDTB[\"date_edited\"])\n",
    "\n",
    "# reorganize new & old cols into desired order\n",
    "all_cols = cols + man_tags + tags\n",
    "ficDTB = ficDTB[all_cols]\n",
    "\n",
    "ficDTB.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200b08d",
   "metadata": {},
   "source": [
    "#### Fic CHECKPOINT! (fic-3-all, all data from prev versions + empty cols ready to be filled by AO3 API)\n",
    "- most recent02-03-23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03022af9",
   "metadata": {},
   "source": [
    "# And the notebook is done here!\n",
    "- Next notebook is focused on competely compiling ALL fics into ficDTB & filling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c31319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
