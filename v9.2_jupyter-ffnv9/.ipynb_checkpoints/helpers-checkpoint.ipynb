{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c63b0e5",
   "metadata": {},
   "source": [
    "# Planning & Helper Functions\n",
    "- <b>Name:</b> Sofia Kobayashi\n",
    "- <b>Date:</b> 12/10/2022\n",
    "- <b>Description:</b> FFN Project planning & helper functions for ffnv9.1 & ffnv9.2 Jupyter Notebook\n",
    "\n",
    "#### <b><u>Functions Table of Contents</u></b>\n",
    "0. [PROJECT PLANNING](#sec1)\n",
    "1. [Cleaning & Incoming Data functions](#sec1)\n",
    "1. [Display & Interface functions](#sec2)\n",
    "1. [Search functions](#sec3)\n",
    "1. [Meta-functions: testing & report](#sec4)\n",
    "1. [Fic functions](#sec5)\n",
    "\n",
    "1. [Misc. functions](#sec6)\n",
    "1. [Single-use functions](#sec7)\n",
    "1. [Cool Code](#sec8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8880637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT STATEMENTS & GLOBAL VARIABLES\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "known_work_types = [\"works\",\"collections\",\"series\",\"users\",\"tags\",\"search\",\"external_works\",\"comments\",\"chapters\"]\n",
    "masterNoDupUrls = \"MASTER_noDupURLs.json\"\n",
    "masterNoDupWorks = \"MASTER_noDupWorks.json\"\n",
    "masterOthers = \"MASTER_others.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813478bb",
   "metadata": {},
   "source": [
    "<a id=\"sec0\"></a>\n",
    "## OVERALL PROJECT PLANNING (ffnv9.2 & beyond)\n",
    "- the 4 goals list in this cell go top -> bottom: concrete/small scale -> overall/big scale\n",
    "\n",
    "\n",
    "**CURRENT: #1** Got all \n",
    "\n",
    "<u>Steps for now</u>\n",
    "1. Compile all text only fics, add url ones to separate file\n",
    "1. Compile all URLs to 1 JSON file, each url has: url, dateAdded, dateLastViewed\n",
    "    - get all, sort out other\n",
    "1. Light analysis\n",
    "    - count how many times each url appears for Safari reading list vs # times work appears\n",
    "    - get all old urls, see how many are missing from current readinglist\n",
    "\n",
    "\n",
    "- <u>Work Types:</u> fic (regular, chapters, colWorks, external_work), user, collection, series, other (search, tag, comments)\n",
    "    - other set-up?: work_type, reason_listed (look into, fav, watch?, etc.), url\n",
    "\n",
    "\n",
    "**Data Section Goals**\n",
    "1. Compiling all URLs and all text fics **[to do]**\n",
    "    1. [ ] Compile all text fics\n",
    "        - v1-6 (in v7), temp updates? \n",
    "    1. [x] Compile all URLs from all lists\n",
    "        - x Safari reading lists (with date added) \n",
    "        - x old local URL txt files\n",
    "        - x Chrome reading lists (date added too?)\n",
    "        - x temp updates/FFN DTB (v7&8)?\n",
    "    1. Clean, combine, de-dup\n",
    "    1. Light analysis\n",
    "2. Creating database architecture **[to do]**\n",
    "    1. [ ] Make flow chart for databases\n",
    "        - Fics: read, to read/cont read, coffee, look into\n",
    "        - Non-fics: Series, authors, collections, other (search, tag, fandom, comments, etc.)\n",
    "            - these non-fics will also get a label (look into,  \n",
    "    1. [ ] Detangle fics-series-collections-look intos \n",
    "3. Getting all possible fic data (clean & formatted) **[finish #2]**\n",
    "    1. [ ] Format all urls -> databases (aka update database process)\n",
    "        1. Pick out all non-AO3 & external works\n",
    "        1. Sort remaining by work_types\n",
    "            - Put into approprite database\n",
    "        1. Get all info for each database (using AO3 API)\n",
    "    1. [ ] Find way to convert text fic info -> url -> databases\n",
    "        - add their info too\n",
    "4. Design my-info input process **[finish #3]**\n",
    "    1. [ ] Decide what info I want to store for each work_type\n",
    "    1. [ ] Create functions for me to see fic/work and add my info, then updates database\n",
    "        - filling in data: full ffn, quick sort? (star, reread, etc.), temp update, coffee\n",
    "        - changing/update works: moving fic from to read -> ffn or something else, look into -> something else\n",
    "5. Creating intake system to quickly add new URLs **[finish #3/4]**\n",
    "    1. [ ] Make function that gets current Safari reading list & updates databases\n",
    "    1. [ ] Make function that takes a txt file of URLs & updates databases\n",
    "6. [ ] Fill-in info **[finish #4]**\n",
    "    \n",
    "\n",
    "\n",
    "**Implementation Goals**\n",
    "1. Data Section - get all the fic info & my rating info, sort into DTB categories, be able to update it easily **[current]**\n",
    "1. Analysis Section - analyze my own reading habits?\n",
    "1. Display Section - display & search for fics & fic lists (website? app?) \n",
    "1. Product/Future Work Section - make something with data (personal algorithm? fandom analysis)\n",
    "    - Personal fic finding algorithm (on existing fics & one for new/upcoming fics)\n",
    "    - Fandom analysis/web crawling functions    \n",
    " \n",
    "\n",
    "\n",
    "**Guiding Goals for Overall FFN Project:**\n",
    "1. Honor favorite fics\n",
    "2. Keep track & easily find favorites/read lists\n",
    "3. Find new ones to read/reread easier\n",
    "4. Get data on fics, fandoms & my own reading habits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd6f02",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## Cleaning & Incoming Data Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73d536",
   "metadata": {},
   "source": [
    "### getTypeAndId() Possibilities\n",
    "<u>input - url possibilities</u>\n",
    "- regular: work, series, authors, collections, tags\n",
    "- colWork, colWork-depricated, search (with ?), chapters (depricated), external_works\n",
    "\n",
    "\n",
    "<u>output - type possibilities</u>\n",
    "- works - id num\n",
    "- series - id num\n",
    "- external_works - id num\n",
    "- comments - id num\n",
    "- chapters - id num\n",
    "- - \n",
    "- collections - name\n",
    "- users - name\n",
    "- tags - tag string\n",
    "- search - long search string\n",
    "- - \n",
    "- (colWork) collections:colName - id num\n",
    "\n",
    "\n",
    "<u>work types</u>\n",
    "1. work: regular, chapters, colWorks\n",
    "1. user\n",
    "1. collection\n",
    "1. series\n",
    "1. search\n",
    "1. tag\n",
    "1. comments\n",
    "1. other: non-ao3, external_work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc1397a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing getting type & id from: \n",
    "url_0 = 'https://archiveofourown.org/collections/WorksOfGreatQualityAcrossTheFandoms/works/29387814'\n",
    "url_1 = 'https://archiveofourown.org/works/22269148/chapters/53178208'\n",
    "url_2 = 'https://archiveofourown.org/users/miscellea/pseuds/The%20Feels%20Whale'\n",
    "url_3 = 'https://archiveofourown.org/collections/TheCrackheadBible/works?commit=Sort+and+Filter&include_work_search%5Bfandom_ids%5D%5B%5D=3828398&page=6&utf8=%E2%9C%93&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=kudos%3A+%26gt%3B1000&work_search%5Bsort_column%5D=kudos_count&work_search%5Bwords_from%5D=25000&work_search%5Bwords_to%5D='\n",
    "url_4 = 'https://archiveofourown.org/external_works/637417'\n",
    "url_5 = 'https://archiveofourown.org/tags/Danny%20Phantom/works'\n",
    "url_6 = \"https://archiveofourown.org/collections/Clever_Crossovers_and_Fantastic_Fusions\"\n",
    "url_7 = \"https://archiveofourown.org/works?commit=Sort+and+Filter&work_search%5Bsort_column%5D=kudos_count&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=&work_search%5Bcomplete%5D=&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=&tag_id=L%C3%A1n+Q%C7%90r%C3%A9n*s*M%C3%A8ng+Y%C3%A1o+%7C+J%C4%ABn+Gu%C4%81ngy%C3%A1o\"\n",
    "url_8 = \"https://archiveofourown.org/works/search?utf8=%E2%9C%93&commit=Search&work_search%5Bquery%5D=&work_search%5Btitle%5D=Assembly+of+Pain%2C+Happiness%2C+%26+Feelings.&work_search%5Bcreators%5D=&work_search%5Brevised_at%5D=&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bsingle_chapter%5D=0&work_search%5Bword_count%5D=&work_search%5Blanguage_id%5D=&work_search%5Bfandom_names%5D=&work_search%5Brating_ids%5D=&work_search%5Bcharacter_names%5D=TommyInnit+%28Video+Blogging+RPF%29&work_search%5Brelationship_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Bcomments_count%5D=&work_search%5Bbookmarks_count%5D=&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc#:~:text=Works%20List-,Assembly%20of%20Pain%2C%20Happiness%2C%20%26%20Feelings.,-by%20RandomlySane\"\n",
    "url_9 = \"https://archiveofourown.org/chapters/747149?show_comments=true\"\n",
    "url_10 = \"https://archiveofourown.org/collections/asoiaftimetraveltransmigration/works/29620161\"\n",
    "url_11 = \"https://archiveofourown.org/bookmarks?commit=Sort+and+Filter&bookmark_search%5Bsort_column%5D=created_at&include_bookmark_search%5Brelationship_ids%5D%5B%5D=27817261&bookmark_search%5Bother_tag_names%5D=&bookmark_search%5Bother_bookmark_tag_names%5D=&bookmark_search%5Bexcluded_tag_names%5D=&bookmark_search%5Bexcluded_bookmark_tag_names%5D=&bookmark_search%5Bbookmarkable_query%5D=&bookmark_search%5Bbookmark_query%5D=&bookmark_search%5Blanguage_id%5D=&bookmark_search%5Brec%5D=0&bookmark_search%5Bwith_notes%5D=0&user_id=kyme\"\n",
    "url_12 = \"https://archiveofourown.org/collections:TheCrackheadBible/15774906\"\n",
    "url_13 = \"https://archiveofourown.org/tags/esama\"\n",
    "\n",
    "import re\n",
    "def getTypeAndId(url):\n",
    "    \"\"\"Give an AO3 url. Returns a tuple with (type-of-work, work-id). Type = works, series, tags, etc.\n",
    "    If type = 'collections', assumed to be a colWork, returns (collections:colName, workId).\n",
    "    Depends on: re\"\"\"\n",
    "    # Check if it's a search result\n",
    "#     print(url)\n",
    "    # [search url]\n",
    "    if (\"works?\" in url) or (\"search?\" in url) or (\"bookmarks?\" in url):\n",
    "        pattern = re.compile('archiveofourown.org/(.+)')\n",
    "        search = pattern.findall(url)[0]\n",
    "        return(\"search\", search)\n",
    "    \n",
    "    # reformatting depricated colWork urls\n",
    "    if \"collections:\" in url:\n",
    "        pattern = re.compile (\"archiveofourown.org/collections:(.+)/(\\d+)\")\n",
    "        search = pattern.findall(url)[0]\n",
    "        url = f'https://archiveofourown.org/collections/{search[0]}/works/{search[1]}'\n",
    "    \n",
    "    # Find work type\n",
    "    pattern = re.compile(\"(archiveofourown.org/)(\\w+)/\")\n",
    "    info = pattern.findall(url)\n",
    "    wType = info[0][1]\n",
    "\n",
    "    # Check if it's an unknown type\n",
    "    if wType not in known_work_types:\n",
    "        raise Exception(f'WorkType not in global variable known_work_types!\\n- url: {url}\\n- output type: {wType}')\n",
    "    \n",
    "    # *** FIND TYPE & ID ***\n",
    "    # If work type is 'collections', I think it has to be a colWork (different URL format)\n",
    "    if wType == \"collections\":\n",
    "        pattern = re.compile(\"archiveofourown.org/collections/(\\w+)/works/(\\d+)\")\n",
    "        info2 = pattern.findall(url)\n",
    "        \n",
    "        # Check if it's a colWork\n",
    "        # [colWork or collections url]\n",
    "        if info2 == []:\n",
    "            pattern = re.compile(\"archiveofourown.org/collections/(.+)$\")\n",
    "            info3 = pattern.findall(url)\n",
    "\n",
    "            # Check if it's NOT a colWork or collection\n",
    "            if info3 == []:\n",
    "                raise Exception(f'type=\"collections\", but not a colWork or collection!\\n- url: {url}\\n- output type: {wType}')\n",
    "             \n",
    "            #Return collection data\n",
    "            return (wType, info3[0])\n",
    "        \n",
    "        # Return colWork data\n",
    "        colName = info2[0][0]\n",
    "        wId = info2[0][1]\n",
    "        return(f\"{wType}:{colName}\", wId)\n",
    "\n",
    "    # [users url]\n",
    "    elif wType == \"users\":\n",
    "        pattern = re.compile(\"archiveofourown.org/users/(\\w+)\")\n",
    "        authorName = pattern.findall(url)[0]\n",
    "        return (wType, authorName)\n",
    "    \n",
    "    # [tags url]\n",
    "    elif wType == \"tags\":\n",
    "        pattern = re.compile(\"archiveofourown.org/tags/(.+)\")\n",
    "        tag = pattern.findall(url)[0]\n",
    "        return (wType, tag)\n",
    "    \n",
    "    # Else, return type & idNum\n",
    "    # [tags url]\n",
    "    else:\n",
    "        pattern = re.compile(\"(archiveofourown.org/)(\\w+)/(\\d+)\")\n",
    "        info = pattern.findall(url)\n",
    "        return (wType, info[0][2])\n",
    "\n",
    "# for i in range(12):\n",
    "#     print(getTypeAndId(url_12))\n",
    "\n",
    "# getTypeAndId(url_13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a416bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def combineToTxt(dirPath):\n",
    "    \"\"\"\n",
    "    Takes a string-path to a directory full of TXT files. Function then combines all files into 1 TXT file,\n",
    "    will only add all text to 1 file, no de-duppinhg.\n",
    "    \"\"\"\n",
    "    # Get date\n",
    "    now = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    # Get all files in given directory\n",
    "    allFiles = get_all_files(dirPath)\n",
    "    others = []\n",
    "\n",
    "    # Get all lines in files\n",
    "    for file in allFiles:\n",
    "        # read in file\n",
    "        with open(f\"{dirPath}/{file}\", \"r\") as infile:\n",
    "            for line in infile:\n",
    "                line.strip() # remove trailing whitespace\n",
    "                others.append(line)\n",
    "\n",
    "    # Write to json\n",
    "    with open(f\"txtOutput_{now}.txt\",\"w\") as outfile:\n",
    "        outfile.writelines(others)\n",
    "    \n",
    "    return f\"txtOutput_{now}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e05560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def combineToJson(dirPath):\n",
    "    \"\"\"Takes a string-path to a directory full of TXT files. Function then combines all files into 1 JSON file.\"\"\"\n",
    "    # Get date\n",
    "    now = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    # Get all files in given directory\n",
    "    allFiles = get_all_files(dirPath)\n",
    "    others = []\n",
    "\n",
    "    # Get all lines in files\n",
    "    for file in allFiles:\n",
    "        # read in file\n",
    "        with open(f\"{dirPath}/{file}\", \"r\") as infile:\n",
    "            for line in infile:\n",
    "                line = line.strip() #to get rid of \\n  \n",
    "                others.append(line)\n",
    "\n",
    "    # Write to json\n",
    "    with open(f\"jsonOutput_{now}.json\",\"w\") as outfile:\n",
    "        json.dump(others, outfile)\n",
    "        \n",
    "    return f\"jsonOutput_{now}.json\"\n",
    "\n",
    "# combineToJson(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c4c8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def txtToJson(fileName):\n",
    "    \"\"\"\n",
    "    Takes a TXT file name & creates a JSON file from the contents.\n",
    "    Returns name of output json file.\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    with open(fileName, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            urls.append(line)\n",
    "    \n",
    "    with open(f\"{fileName.replace('.txt','')}.json\", \"w\") as outfile:\n",
    "        json.dump(urls, outfile)\n",
    "    \n",
    "    return f\"{fileName.replace('.txt','')}.json\"\n",
    "\n",
    "# txtToJson(\"urlsOutput/v8_chrome.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8b643be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def add_to_masterfiles(urlFile):\n",
    "    \"\"\"\n",
    "    Takes ONE txt file of URLs, appends new urls (probably from a new reading list) to the 3 MASTER json files:\n",
    "    MASTER_noDupURLs, MASTER_noDupWorks, MASTER_others. Pair with `combineToTxt(dirPath)` to convert whole \n",
    "    folders of TXT files.\n",
    "    Returns 'success' if successful. \n",
    "    \"\"\"\n",
    "    # Get current date & initialize 3 lists\n",
    "    now = datetime.now()\n",
    "    date_str = f\"<Added: {now.strftime('%m-%d-%y %H:%M:%S')}>\"\n",
    "    \n",
    "    \n",
    "    # Initialize variables\n",
    "    files = [\"MASTER_noDupURLs.json\", \"MASTER_noDupWorks.json\", \"MASTER_others.json\"]\n",
    "    \n",
    "    for file in files:\n",
    "        if not os.path.isfile(file):\n",
    "            with open(file,\"w\") as outfile:\n",
    "                json.dump([], outfile)\n",
    "            print(f\"Made {file}\")\n",
    "    \n",
    "    newNoDupURLs = []\n",
    "    newNoDupWorks = []\n",
    "    newOthers = []\n",
    "\n",
    "    \n",
    "    # Read in original files\n",
    "    with open(files[0], \"r\") as infile:\n",
    "        noDupURLs = json.load(infile)\n",
    "    \n",
    "    # rules a little different for noDupWorks bc it's formatted: [[typeI, url], ...\n",
    "    with open(files[1], \"r\") as infile:  \n",
    "        noDupWorks = json.load(infile)\n",
    "        if noDupWorks == []: typeIdList = []\n",
    "        else: \n",
    "            typeIdList = list(list(zip(*noDupWorks))[0])\n",
    "        \n",
    "    with open(files[2], \"r\") as infile:\n",
    "        others = json.load(infile)\n",
    "\n",
    "    totalLen = 0\n",
    "    # Read in new URLs \n",
    "    with open(urlFile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip() #to get rid of \\n\n",
    "#             print(line) #DID SOMETHING GO WRONG?\n",
    "            # if not an AO3 url\n",
    "            # 1. others filter\n",
    "            if \"archiveofourown.org\" not in line:\n",
    "                if line not in others:\n",
    "                    newOthers.append(line)\n",
    "\n",
    "            else:\n",
    "                # 2. noDupUrls filter\n",
    "                if line not in noDupURLs:\n",
    "                    newNoDupURLs.append(line)\n",
    "\n",
    "                # 3. noDupWorks filter\n",
    "                typeId = list(getTypeAndId(line))\n",
    "                if typeId not in typeIdList:\n",
    "                    typeIdList.append(typeId)\n",
    "                    pair = [typeId, url]\n",
    "                    newNoDupWorks.append(pair)\n",
    "            totalLen += 1\n",
    "    \n",
    "    # Format & Write newly added-to files\n",
    "    fileTypes = [[noDupURLs, newNoDupURLs, files[0]], \n",
    "                 [noDupWorks, newNoDupWorks, files[1]], \n",
    "                 [others, newOthers, files[2]]]\n",
    "        \n",
    "    for original, new, file in fileTypes:\n",
    "        original.append(date_str) # add date stamp\n",
    "        original.extend(new) # add new URLs\n",
    "        \n",
    "        # Write newly appended-lists\n",
    "        with open(file, \"w\") as infile:\n",
    "            json.dump(original, infile)\n",
    "        \n",
    "    # print addition report\n",
    "    print(f\"There were {totalLen} url(s) in '{urlFile}'\")\n",
    "    print(f\"Added {len(newNoDupURLs)} url(s) to MASTER_noDupURLs.json\")\n",
    "    print(f\"Added {len(newNoDupWorks)} url(s) to MASTER_noDupWorks.json\")\n",
    "    print(f\"Added {len(newOthers)} url(s) to MASTER_others.json\")\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "    \n",
    "# add_to_masterfiles(\"txtOutput_12-25-22.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226e31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_to_masterfiles(dirPath):\n",
    "    \"\"\"\n",
    "    Takes a directory (full of URL TXT files), makes a combined TXT files, then adds all those URLs to MASTER\n",
    "    files.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    txtFile = combineToTxt(dirPath)\n",
    "    result = add_to_masterfiles(txtFile)\n",
    "    print(f\"{result.title()}!\")\n",
    "\n",
    "# all_to_masterfiles(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581f55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readinglist_to_masterfiles():\n",
    "    \"\"\"\n",
    "    Makes TXT file from Safari reading list, adds all those urls to master files.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    txtFile = getReadingList()\n",
    "    result = add_to_masterfiles(txtFile)\n",
    "    print(f\"{result.title()}!\")\n",
    "\n",
    "# readinglist_to_masterfiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e267c5",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## Display & Interface functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30653c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectInput(allowedList):\n",
    "    \"\"\"Gets & returns user input but keeps prompting user until input is within correctList.\"\"\"\n",
    "    # run loop until input is within correctList\n",
    "    passes = False\n",
    "    while not passes: \n",
    "        passes = True\n",
    "        ans = input()\n",
    "        if ans not in allowedList:\n",
    "            passes = False\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebf0d1",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db2be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    import difflib\n",
    "    difflib.get_close_matches(\"apple\", [\"apl\", \"app\", \"bee\", \"cici\"], n=3, cutoff=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989095a",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## Meta-functions: Testing & Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6fb7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reportMasters():\n",
    "    for file in [masterNoDupUrls, masterNoDupWorks, masterOthers]:\n",
    "        with open(file) as infile:\n",
    "            data = json.load(infile)\n",
    "            print(f\"{file} has {len(data)} url(s)\")\n",
    "            \n",
    "# reportMasters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9585d46",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## Fic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997164c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08913ca8",
   "metadata": {},
   "source": [
    "<a id=\"sec6\"></a>\n",
    "## Misc. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5806a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AO3\n",
    "\n",
    "def my_session():\n",
    "    \"\"\"\n",
    "    Returns an AO3 session logged in to my account.\n",
    "    \"\"\"\n",
    "    payload = open(\"randomData/to_add_authors.txt\", \"r\")\n",
    "    user = payload.readline().strip()\n",
    "    password = payload.readline().strip()\n",
    "    \n",
    "    sess = AO3.Session(user, password)\n",
    "    sess.refresh_auth_token()\n",
    "    payload.close()\n",
    "    \n",
    "    return sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9f0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def get_all_files(dirName): \n",
    "    \"\"\"Takes a string - name of directory. Returns list of ALL files within that directory minus the .DS_Store\"\"\"\n",
    "    allFiles = [f for f in listdir(dirName)]\n",
    "    if \".DS_Store\" in allFiles:\n",
    "        allFiles.remove(\".DS_Store\")\n",
    "    return allFiles\n",
    "\n",
    "# get_all_files(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ff8f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReadingList():\n",
    "    \"\"\"extracturls.py ~ This script gets a list of all the URLs in Safari Reading List, and\n",
    "    writes them all to a file. Requires Python 3. ~ from someone on StackOverflow\"\"\"\n",
    "    #!/usr/bin/env python\n",
    "    import os\n",
    "    import plistlib\n",
    "\n",
    "    # Get current date \n",
    "    now = datetime.now()\n",
    "    current_date = now.strftime(\"%m-%d-%y\")\n",
    "\n",
    "    # set file paths\n",
    "    INPUT_FILE  = os.path.join(os.environ['HOME'], 'Library/Safari/Bookmarks.plist')\n",
    "    OUTPUT_FILE = f\"readinglist_{current_date}.txt\"\n",
    "\n",
    "    # Load and parse the Bookmarks file\n",
    "    with open(INPUT_FILE, 'rb') as plist_file:\n",
    "        plist = plistlib.load(plist_file)\n",
    "\n",
    "    # Look for the child node which contains the Reading List data.\n",
    "    # There should only be one Reading List item\n",
    "    children = plist['Children']\n",
    "    for child in children:\n",
    "        if child.get('Title', None) == 'com.apple.ReadingList':\n",
    "            reading_list = child\n",
    "\n",
    "    # Extract the bookmarks\n",
    "    bookmarks = reading_list['Children']\n",
    "\n",
    "    # For each bookmark in the bookmark list, grab the URL\n",
    "    urls = (bookmark['URLString'] for bookmark in bookmarks)\n",
    "\n",
    "    # Write the URLs to a file\n",
    "    with open(OUTPUT_FILE, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(urls))\n",
    "    \n",
    "    print(f\"Wrote to {OUTPUT_FILE}\")\n",
    "    return OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e670ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOnlyAO3(fileName):\n",
    "    \"\"\"DEPRICATED - Takes a TXT file of URLs & returns 2 lists: [[the AO3 link], [non-AO3 links]]\"\"\"\n",
    "    # Read in file of URLs\n",
    "    with open(fileName, \"r\") as infile:\n",
    "        lines = infile.readlines()\n",
    "    \n",
    "    # Sort URLs into archive & non-archive lists \n",
    "    archive = []\n",
    "    notArchive = []\n",
    "    for line in lines:\n",
    "        if \"archiveofourown.org\" in line:\n",
    "            archive.append(line)\n",
    "        else:\n",
    "            notArchive.append(line)\n",
    "\n",
    "    return [archive, notArchive]\n",
    "    \n",
    "    \n",
    "#     # Write archive URLs to file\n",
    "#     fileNice = fileName.split(\"/\")[-1] \\\n",
    "#                         .replace(' ','-')\n",
    "#     archiveFile = f\"archive_{fileNice}\"\n",
    "#     with open(archiveFile, \"w\") as outfile:\n",
    "#         outfile.writelines(archive)\n",
    "#         print(f\"Wrote {len(archive)} AO3 link(s) to {archiveFile}\")\n",
    "    \n",
    "#     # Write non-archive URLs to file\n",
    "#     notArchiveFile = f\"notArchive_{fileNice}\"\n",
    "#     with open(notArchiveFile, \"w\") as outfile:\n",
    "#         outfile.writelines(notArchive)\n",
    "#         print(f\"Wrote {len(notArchive)} non-AO3 links to {notArchiveFile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c4281",
   "metadata": {},
   "source": [
    "<a id=\"sec7\"></a>\n",
    "## Single-use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "042e297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCol():\n",
    "    \"\"\"Single use - Made Collection Works URLs from (idNum, collectionName) tuples.\"\"\"\n",
    "    # Reads in numColWorks TXT file\n",
    "    res = []\n",
    "    with open(\"urls/numColWorks.txt\", \"r\") as inFile:\n",
    "        lines = inFile.readlines()\n",
    "        \n",
    "    # Creates ColWorks URLs from the id & name\n",
    "    for line in lines:\n",
    "        data = line[:-1].split(\",\")\n",
    "        col = data[1]\n",
    "        idNum = data[0]\n",
    "        print(f\"https://archiveofourown.org/collections/{col}/works/{idNum}\")\n",
    "\n",
    "    # Writes ColWorks URLs\n",
    "    fileName = f\"urlsOutput/from_{'numColWorks'.lower().replace(' ','-')}.txt\"\n",
    "    with open(fileName, \"w\") as outFile:\n",
    "        print(f\"Writing to {fileName}\")\n",
    "        outFile.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4ba04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToStart(infile, strToAdd):\n",
    "    \"\"\"Single Use (kinda) - Takes a TXT file & string. Adds given string to the front of each line in the file, \n",
    "    writes a new file named 'from_{given file}'.\"\"\"\n",
    "    # Reads in given file\n",
    "    res = []\n",
    "    with open(f\"urls/urlFiles/{infile}\", \"r\") as inFile:\n",
    "        lines = inFile.readlines()\n",
    "        \n",
    "    # Attaches given string to front\n",
    "    for line in lines:\n",
    "        res.append(strToAdd+line)\n",
    "\n",
    "    # Overwrites given file with new \n",
    "    with open(f\"urlsOutput/from_{infile.lower().replace(' ','-')}\", \"w\") as outFile:\n",
    "        outFile.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "786c3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate dual-urls (FROM 1_dtbs_from_prev)\n",
    "if False: \n",
    "    print(f\"len pre: {len(pre)}\")\n",
    "    url = \"https://archiveofourown.org/works/9841367/chapters/22088246https://archiveofourown.org/works/76861\"\n",
    "\n",
    "    import re\n",
    "    for url in pre: \n",
    "        pattern = re.compile('.+(https://archiveofourown.org/.+)')\n",
    "        search = pattern.findall(url)\n",
    "        if len(search) > 0:\n",
    "            start = url.find(search[0])\n",
    "            url1 = url[:start]\n",
    "            url2 = url[start:]\n",
    "            pre.remove(url)\n",
    "            pre.append(url1)\n",
    "            pre.append(url2)\n",
    "        \n",
    "# print(f\"len pre after: {len(pre)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89db7c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-use, converting v1-6 text fics to json from txt \n",
    "if False: \n",
    "    v16_txt_fics = []\n",
    "    with open(\"urlsOutput/v1-6_txt/all_early.txt\", \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if 'ver.' not in line.lower():\n",
    "                info = line.split('---')\n",
    "\n",
    "                temp = {}\n",
    "                temp[\"version\"] = int(info[0])\n",
    "                temp[\"title\"] = str(info[1])\n",
    "                v16_txt_fics.append(temp)\n",
    "\n",
    "\n",
    "    with open(f\"urlsOutput/v1-6_txt/all_early_1.json\", \"w\") as outfile:\n",
    "        json.dump(v16_txt_fics, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3275e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single-use, making CSV file for unsorted txt fics (v1-6)\n",
    "if False: \n",
    "    with open(\"urlsOutput/v1-6_txt/all_early.json\", \"r\") as infile:\n",
    "        early_unsorted = json.load(infile)\n",
    "\n",
    "    t1 = pd.DataFrame(early_unsorted)\n",
    "\n",
    "    def temp(x):\n",
    "        if \"series\" in x.lower(): return \"series\"\n",
    "        else: return 'fic'\n",
    "\n",
    "    def temp2(x):\n",
    "        str_date = version_default_dates[int(x)]\n",
    "        date_date = datetime.strptime(str_date, '%m-%d-%Y %H:%M:%S')\n",
    "        return date_date\n",
    "\n",
    "    t1[\"work_type\"] = t1[\"title\"].apply(temp)\n",
    "    t1[\"smk_source\"] = t1[\"version\"].apply(lambda x: f\"v{x}_list\")\n",
    "    t1[\"dtb_type\"] = \"read\"\n",
    "    t1[\"date_added\"] = t1[\"version\"].apply(temp2)\n",
    "    t1['is_sorted'] = False\n",
    "\n",
    "    # t1.to_csv('all_early_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "193673e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fic url to fic dtb\n",
    "def urlToFicDTB(url):\n",
    "    \"\"\"\n",
    "    Outdated - Takes an AO3 fic url, creates a new row with appropriate info.\n",
    "    Returns that new row with generated info.\n",
    "    \"\"\"\n",
    "    row_data = {'dtb_type': [\"read\"], \n",
    "               'smk_source': [\"v7_sheets\"], \n",
    "               'version': [7], \n",
    "               'date_added': [datetime.strptime(\"01-01-22 00:00:01\", \"%m-%d-%y %H:%M:%S\")], \n",
    "               'date_last_viewed': [np.datetime64(\"NaT\")],\n",
    "               'url_type': [getTypeAndId(url)[0]], \n",
    "               'id': [getTypeAndId(url)[1]], \n",
    "               'url': [url]\n",
    "              }\n",
    "\n",
    "    new_row = pd.DataFrame(data=row_data)\n",
    "\n",
    "\n",
    "# total_4 = pd.concat([total_4, new_row], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b97b8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_combine(df1, df2, debug=False):\n",
    "    \"\"\"\n",
    "    Takes 2 data frames, de-ups them, & updates (not just .update) df1 with df2.\n",
    "    Since .update aligns on index, must fill both res & df2 with all rows.\n",
    "    Returns updated df.\n",
    "    \"\"\"\n",
    "    # concat & de-dup (keeping first), sort & reset index (since update aligns on index)\n",
    "    res = pd.concat([df1, df2]) \\\n",
    "            .drop_duplicates(subset=[\"url\"],keep=\"first\") \\\n",
    "            .sort_values(\"url\") \\\n",
    "            .reset_index()\n",
    "    \n",
    "    if debug == True: print(f\"-- {res.smk_source.value_counts()}\")\n",
    "    \n",
    "    df2 = pd.concat([df2, df1]) \\\n",
    "            .drop_duplicates(subset=[\"url\"],keep=\"first\") \\\n",
    "            .sort_values(\"url\") \\\n",
    "            .reset_index()\n",
    "    if debug == True: print(f\"-- {df2.smk_source.value_counts()}\")\n",
    "    \n",
    "    \n",
    "    # update with df2\n",
    "    res.update(df2)\n",
    "    res = res.drop(columns=[\"index\"])\n",
    "    \n",
    "    #ensure no duplicates\n",
    "    if len(res) != len(res.drop_duplicates(subset=[\"url\"])):\n",
    "        raise Exception(f\"Duplicates? {len(res)} in res, {res.drop_duplicates(subset=['url'])} after drop_duplicates\")\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "dfdfb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateInfo(dtb, keyColName, keyInfo, newColName, newInfo, debug=True):   \n",
    "    \"\"\"\n",
    "    Takes a DF & 2 pairs of parameters: 1. colName & value of 'key' to identify row to be changed and \n",
    "                                        2. colName & value of info to get updated in that row\n",
    "    Key must be unique.\n",
    "    Returns 0 if failed, 1 if success.\n",
    "    \"\"\"\n",
    "    ind = dtb.index[dtb[keyColName] == keyInfo].tolist()\n",
    "    if len(ind) == 0:\n",
    "        if debug: print(f\"- ERROR: '{keyInfo}' not found in '{keyColName}' column!\")\n",
    "        return 0\n",
    "    elif len(ind) > 1:\n",
    "        if debug: print(f\"- ERROR: multiple of given key info found in dtb! Key must be unique!\")\n",
    "        return 0\n",
    "    else:\n",
    "        before = dtb.at[ind[0], newColName]\n",
    "        dtb.at[ind[0], newColName] = newInfo\n",
    "        if debug: print(f\"Updated cell [#{ind[0]}, '{newColName}']: '{before}' -> '{newInfo}'\")\n",
    "        return 1\n",
    "\n",
    "# updateInfo(t1, 'url', \"https://archiveofourown.org/series/1995952\", \"name\", \"Shifters\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b42191",
   "metadata": {},
   "source": [
    "<a id=\"sec8\"></a>\n",
    "## Cool Code/Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3b3326",
   "metadata": {},
   "source": [
    "### Querying Examples - SQL & .query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc705ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying examples\n",
    "if False: \n",
    "    # pd.df.query testing\n",
    "    work_t = \"series\"\n",
    "    id_n = '1029669'\n",
    "    col = \"url\"\n",
    "\n",
    "    df1.query(f\"url_type == '{work_t}' and id == '{id_n}'\").iloc[0][col]\n",
    "\n",
    "\n",
    "    # pd sql testing\n",
    "    import pandas as pd\n",
    "    import pandasql as ps\n",
    "\n",
    "    data = df1\n",
    "    sql_query = \"SELECT * FROM data WHERE work_type == 'work' AND date_added == '2022-01-11'\"\n",
    "    ps.sqldf(sql_query, locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7fbb36",
   "metadata": {},
   "source": [
    "### DF Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6c00d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    no = np.datetime64(\"NaT\")\n",
    "\n",
    "    df1 = pd.DataFrame({\"url\": [\"a\",\"b\",\"c\",\"e\",'f'],\n",
    "                       \"date_added\": [1,np.nan,3,7,8],\n",
    "                       \"date_last_viewed\": [no,no,no,no,no],\n",
    "                       \"dtb_type\":[np.nan,np.nan,np.nan,np.nan,np.nan],\n",
    "                       \"smk_source\": [\"v8_old\",\"v8_old\",\"v8_old\",\"v8_old\",\"v8_old\"]})\n",
    "    df2 = pd.DataFrame({\"url\": [\"a\",\"b\",\"c\",\"d\"],\n",
    "                   \"date_added\": [4,2,3,6],\n",
    "                   \"date_last_viewed\": [5, no, no, 9],\n",
    "                   \"dtb_type\": [\"read\", \"lookInto\", np.nan, \"toRead\"],\n",
    "                   \"smk_source\": [\"chrome\",\"chrome\",\"chrome\",\"chrome\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dd3c6",
   "metadata": {},
   "source": [
    "### BeautifulSoup example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1075580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS example\n",
    "if False:\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    url = \"https://archiveofourown.org/chapters/17156026?show_comments=true\"\n",
    "\n",
    "    # Getting HTML of the AO3 page\n",
    "    html_text = requests.get(url).text\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "    soup.find('input', attrs={'id': 'kudo_commentable_id','type':'hidden'})[\"value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66d10687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147c4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_series_dtb_original(initial_series_dtb, session, report=False):\n",
    "    \"\"\"\n",
    "    Takes a seriesDTB (pandas DataFrame, post series-0), an AO3 session, and a Boolean to print report.\n",
    "    Modifies given seriesDTB by filling it up with ao3 information.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    # find total number of series to fill\n",
    "    total = max(initial_series_dtb.index)\n",
    "    \n",
    "    # ensure initial_series_dtb has all necessary columns\n",
    "    for col in series_columns:\n",
    "        if col not in initial_series_dtb.columns:\n",
    "            initial_series_dtb[col] = np.nan\n",
    "\n",
    "    for ind in initial_series_dtb.index: # for every row in initial_series_dtb\n",
    "        try: \n",
    "            if not report:\n",
    "                if ind%100 == 0: \n",
    "                    print(f'- {ind}! (printed every 100)')\n",
    "\n",
    "            if not series_row_complete(initial_series_dtb.iloc[[ind]]): # if any null value in rows (sans last_viewed & is_subbed)\n",
    "                # get series id\n",
    "                series_id = initial_series_dtb.at[ind, \"id\"]\n",
    "                if report: print(f\"{ind}: [{(ind/total)*100:.2f}%] Filling for [{series_id}]\")\n",
    "\n",
    "                # initialize Series obj\n",
    "                series = AO3.Series(series_id, session=session)\n",
    "\n",
    "                # write report info\n",
    "                name = series.name\n",
    "                creators = json.dumps([user.username for user in series.creators])\n",
    "                fandoms = json.dumps(getSeriesFandoms(series))\n",
    "\n",
    "                initial_series_dtb.at[ind, \"name\"] = name\n",
    "                initial_series_dtb.at[ind, \"creators\"] = creators\n",
    "                initial_series_dtb.at[ind, \"fandoms\"] = fandoms\n",
    "                if report: print(f\"- Wrote '{name}' by {creators}\\nin {fandoms}\")\n",
    "\n",
    "                # write remaining info\n",
    "                initial_series_dtb.at[ind, \"series_obj\"] = series\n",
    "                initial_series_dtb.at[ind, \"date_obj_updated\"] = datetime.now()\n",
    "\n",
    "                initial_series_dtb.at[ind, \"description\"] = series.description\n",
    "                initial_series_dtb.at[ind, \"notes\"] = series.notes\n",
    "                initial_series_dtb.at[ind, \"words\"] = series.words\n",
    "                initial_series_dtb.at[ind, \"complete\"] = series.complete\n",
    "                initial_series_dtb.at[ind, \"is_subscribed\"] = series.is_subscribed\n",
    "\n",
    "                initial_series_dtb.at[ind, \"series_begun\"] = series.series_begun\n",
    "                initial_series_dtb.at[ind, \"series_updated\"] = series.series_updated\n",
    "                initial_series_dtb.at[ind, \"nbookmarks\"] = series.nbookmarks\n",
    "                initial_series_dtb.at[ind, \"nworks\"] = series.nworks\n",
    "                initial_series_dtb.at[ind, \"work_list\"] = json.dumps([work.id for work in get_series_work_list(series)])\n",
    "\n",
    "                initial_series_dtb.at[ind, \"is_restricted\"] = series._soup.find(\"img\", {\"title\": \"Restricted\"}) is not None\n",
    "                initial_series_dtb.at[ind, \"not_found\"] = False\n",
    "            else: \n",
    "                if report: print(f\"{ind}: .\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            initial_series_dtb.at[ind, \"not_found\"] = True\n",
    "            print(f\"-- ERROR, {ind}: {initial_series_dtb.at[ind, 'id']}\")\n",
    "        \n",
    "        # update temp csv w/ new row/series\n",
    "        initial_series_dtb.to_csv(\"temp_series.csv\")\n",
    "\n",
    "    # Write finished series DTB to csv\n",
    "    initial_series_dtb.to_csv(\"temp_series_final.csv\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\nDONE!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
