{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8ad3ec",
   "metadata": {},
   "source": [
    "# FFN Analysis\n",
    "- <b>Date:</b> 10/24/2022\n",
    "- <b><u>Table of Contents</u></b>\n",
    "    1. [Planning](#sec1)\n",
    "    1. [Title1](#sec2)\n",
    "    1. [Title1](#sec3)\n",
    "    1. [Title1](#sec4)\n",
    "    1. [Title1](#sec5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ac608",
   "metadata": {},
   "source": [
    "### Goal #1: Write a function that takes a reading list\n",
    "- separates AO3 link and everything else\n",
    "- checks for duplicates in existing storages & de-dups\n",
    "- separates explicit and non-explicit \n",
    "- grabs all data for each fic & writes it to a csv of 'unsorted'\n",
    "\n",
    "### Goal #2: Write sorting function \n",
    "- that displays one fic from either explicit or non-explicit\n",
    "- take user input, add dtb tag & reread boolean & important\n",
    "    - ffn, to read, author, tag, coffee, cont read, unsure\n",
    "    - reread?\n",
    "    - star\n",
    "- \n",
    "\n",
    "\n",
    "- ao3 id\n",
    "- fandoms []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f61c4b3",
   "metadata": {},
   "source": [
    "### Goals (current)\n",
    "    1. [Clean-Incoming-Data functions] organizer & create fic csv/json forms \n",
    "        \n",
    "    1. [Display & Interface functions] sort reading list URLs, get masterlist of works, authors, tags, others?\n",
    "        - [ ] def getInput(allowedList)  <--- current\n",
    "        - [ ] def read_dtb(dtbName)\n",
    "        - [ ] def display_fic(ficId)\n",
    "        - [ ] def start_sort_temp()\n",
    "        - nice interface, main caller\n",
    "\n",
    "- maybe sort out/differently store series\n",
    "    1. [Search functions] function to go through unsorted lists to sort them into dtbs (NOT in-depth)\n",
    "        - [ ] def saveSort(newInfo)\n",
    "        - [ ] def edit_fic(ficId)\n",
    "        - have an edit method as well\n",
    "        - have a history?\n",
    "        - have a report about all lists\n",
    "    1. [Cleaner & Testing functions]\n",
    "        - def check_all_dtb()\n",
    "            - for cleaning (?), check for duplicates\n",
    "            - def dtb_check_dups(dtbName)\n",
    "        - def dtb_report()\n",
    "            - prints out/writes (?) number of fics, names of recently added? \n",
    "    1. [Fic functions]\n",
    "        - def fic_identify(type, info)\n",
    "            - type: title, id, url\n",
    "            - returns fic_id\n",
    "        - def fic_report(fic_id)\n",
    "            - print report: current dtb, date moved, fic info (title, author, fandom, etc.)\n",
    "        - def interate_author(author_name)\n",
    "            - gets all works not already in dtb of author, displays each other, user chooses if/where they want to add it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6ddcc",
   "metadata": {},
   "source": [
    "### Imports, Variables, & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931af726",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d35fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "key_shorts = {\"f\": \"ffn\",\n",
    "            \"tr\": \"to_read\",\n",
    "            \"cr\": \"continue_read\",  \n",
    "            \"c\": \"coffee\",\n",
    "            \"d\": \"unsure\"\n",
    "             }\n",
    "# author, favorite_tag, explore_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d5ba5e",
   "metadata": {},
   "source": [
    "### General Workspace\n",
    "\n",
    "## CURRENT GOAL: Make pd DF/csv for all MASTERS\n",
    "- Make other one a single-use & store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9752e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def add_to_masterfiles(urlFile):\n",
    "    \"\"\"\n",
    "    Takes ONE txt file of URLs, appends new urls (probably from a new reading list) to the 3 MASTER json files:\n",
    "    MASTER_noDupURLs, MASTER_noDupWorks, MASTER_others. Pair with `combineToTxt(dirPath)` to convert whole \n",
    "    folders of TXT files.\n",
    "    Returns 'success' if successful. \n",
    "    \"\"\"\n",
    "    # Get current date & initialize 3 lists\n",
    "    now = datetime.now()\n",
    "    date_str = f\"<Added: {now.strftime('%m-%d-%y %H:%M:%S')}>\"\n",
    "    \n",
    "    \n",
    "    # Initialize variables\n",
    "    files = [\"MASTER_noDupURLs.json\", \"MASTER_noDupWorks.json\", \"MASTER_others.json\"]\n",
    "    \n",
    "    for file in files:\n",
    "        if not os.path.isfile(file):\n",
    "            with open(file,\"w\") as outfile:\n",
    "                json.dump([], outfile)\n",
    "            print(f\"Made {file}\")\n",
    "    \n",
    "    newNoDupURLs = []\n",
    "    newNoDupWorks = []\n",
    "    newOthers = []\n",
    "\n",
    "    \n",
    "    # Read in original files\n",
    "    with open(files[0], \"r\") as infile:\n",
    "        noDupURLs = json.load(infile)\n",
    "    \n",
    "    # rules a little different for noDupWorks bc it's formatted: [[typeI, url], ...\n",
    "    with open(files[1], \"r\") as infile:  \n",
    "        noDupWorks = json.load(infile)\n",
    "        if noDupWorks == []: typeIdList = []\n",
    "        else: \n",
    "            typeIdList = list(list(zip(*noDupWorks))[0])\n",
    "        \n",
    "    with open(files[2], \"r\") as infile:\n",
    "        others = json.load(infile)\n",
    "\n",
    "    totalLen = 0\n",
    "    # Read in new URLs \n",
    "    with open(urlFile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip() #to get rid of \\n\n",
    "#             print(line) #DID SOMETHING GO WRONG?\n",
    "            # if not an AO3 url\n",
    "            if \"archiveofourown.org\" not in line:\n",
    "                if line not in others:\n",
    "                    newOthers.append(line)\n",
    "\n",
    "            else:\n",
    "                # should add to no-duplicate-url file?\n",
    "                if line not in noDupURLs:\n",
    "                    newNoDupURLs.append(line)\n",
    "\n",
    "                # should add to no-duplicate-work file?\n",
    "                typeId = list(getTypeAndId(line))\n",
    "                if typeId not in typeIdList:\n",
    "                    typeIdList.append(typeId)\n",
    "                    pair = [typeId, url]\n",
    "                    newNoDupWorks.append(pair)\n",
    "            totalLen += 1\n",
    "    \n",
    "    # Format & Write newly added-to files\n",
    "    fileTypes = [[noDupURLs, newNoDupURLs, files[0]], \n",
    "                 [noDupWorks, newNoDupWorks, files[1]], \n",
    "                 [others, newOthers, files[2]]]\n",
    "        \n",
    "    for original, new, file in fileTypes:\n",
    "        original.append(date_str) # add date stamp\n",
    "        original.extend(new) # add new URLs\n",
    "        \n",
    "        # Write newly appended-lists\n",
    "        with open(file, \"w\") as infile:\n",
    "            json.dump(original, infile)\n",
    "        \n",
    "    # print addition report\n",
    "    print(f\"There were {totalLen} url(s) in '{urlFile}'\")\n",
    "    print(f\"Added {len(newNoDupURLs)} url(s) to MASTER_noDupURLs.json\")\n",
    "    print(f\"Added {len(newNoDupWorks)} url(s) to MASTER_noDupWorks.json\")\n",
    "    print(f\"Added {len(newOthers)} url(s) to MASTER_others.json\")\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "    \n",
    "# add_to_masterfiles(\"txtOutput_12-25-22.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9566152",
   "metadata": {},
   "source": [
    "## Prelim-sorting Interface workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2929fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInput():\n",
    "    \"\"\" Gets all the user answers for the v9.1 prelim sorting interface\n",
    "    Takes nothing. Returns nothing.\"\"\"\n",
    "    passes = False\n",
    "    \n",
    "    # get dtb input\n",
    "    print(\"Database: \")\n",
    "    getCorrectInput(list(key_shorts.keys()))\n",
    "    \n",
    "    # get reread input\n",
    "    print(\"Reread?: \")\n",
    "    getCorrectInput([\"r\",\"\"])\n",
    "    \n",
    "    # get star input\n",
    "    print(\"Star?: \")\n",
    "    getCorrectInput([\"s\",\"\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72526a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFic():\n",
    "    # display stuff\n",
    "    print(\"here\")\n",
    "    # get input\n",
    "    dtb = \"-1\"\n",
    "    reread = \"-1\"\n",
    "    star = \"-1\"\n",
    "    \n",
    "    passes = 0\n",
    "    while passes == 0:\n",
    "        passes = 1\n",
    "        ans = getInput()\n",
    "        dtb = ans[0].replace(\" \",\"\").split()\n",
    "        for dtb_tag in dtb:\n",
    "            if dtb_tag not in key_shorts.keys():\n",
    "                print(f\" - '{dtb_tag}' not in key shortcuts\")\n",
    "                passes = 0\n",
    "        if ans[1] != \"r\" or ans[1] != None:\n",
    "            print(f\" - it's 'r' or nothing! (for reread)\")\n",
    "        if ans[2] != \"s\" or ans[2] != \"\":\n",
    "            print(f\" - it's 's' or nothing! (for star)\")\n",
    "        if passes == 0:\n",
    "            print(f\" - shortcuts: {key_shortcuts}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb6a56",
   "metadata": {},
   "source": [
    "## AO3 Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AO3\n",
    "url = \"https://archiveofourown.org/works/13273611/chapters/30371190\"\n",
    "workid = AO3.utils.workid_from_url(url)\n",
    "work = AO3.Work(workid)\n",
    "\n",
    "attrs = dir(work)[33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e24d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "with open(\"newRemain.json\") as infile:\n",
    "    newRemain = json.load(infile)\n",
    "\n",
    "\n",
    "temp = []\n",
    "with open(masterNoDupUrls) as infile:\n",
    "    data = json.load(infile)\n",
    "    for line in data:\n",
    "        if \"<Added:\" not in line:\n",
    "            temp.append(line)\n",
    "\n",
    "def ten(x):\n",
    "    if x in newRemain: return datetime.datetime(2022, 1, 11)\n",
    "    else: return datetime.datetime(2017, 5, 31)\n",
    "        \n",
    "df1 = pd.DataFrame(temp)\n",
    "df1 = df1.rename(columns={0: \"url\"})\n",
    "df1[\"date_added\"] = df1[\"url\"].apply(ten)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f946381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yu(x):\n",
    "    return getTypeAndId(x)[1]\n",
    "\n",
    "df1[\"id\"] = df1[\"url\"].apply(yu)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xu(x):\n",
    "    return getTypeAndId(x)[0]\n",
    "\n",
    "df1[\"type\"] = df1[\"url\"].apply(xu)\n",
    "df1[\"type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeToWorkType(x):\n",
    "    if x == \"users\": return \"user\"\n",
    "    elif x == \"collections\": return \"collection\"\n",
    "    elif x == \"search\": return \"search\"\n",
    "    elif x == \"tags\": return \"tag\"\n",
    "    elif x == \"series\": return \"series\"\n",
    "    elif x == \"works\": return \"work\"\n",
    "    elif x == \"chapters\": return \"work\"\n",
    "    elif x == \"external_works\": return \"work\"\n",
    "    elif 'collections:' in x: return \"work\"\n",
    "    elif x == \"comments\": return \"comments\"\n",
    "    else: return \"UNDEFINED\"\n",
    "    \n",
    "df1[\"work_type\"] = df1[\"type\"].apply(zu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f5781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={\"type\": \"url_type\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1[\"id\"] == \"thepartyresponsible\"][\"url\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddf8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1.to_csv(\"MASTER_noDupURLs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dba8de",
   "metadata": {},
   "source": [
    "collection, work, series, user, tags, search, col_work, external_work, comments, chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1d0f1",
   "metadata": {},
   "source": [
    "add pseuds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad01e5",
   "metadata": {},
   "source": [
    "<u>url_type</u>\n",
    "1. work\n",
    "1. chapters\n",
    "1. colWorks\n",
    "1. external work\n",
    "1. user\n",
    "1. collection\n",
    "1. search\n",
    "1. tag\n",
    "1. series\n",
    "\n",
    "<u>work_type</u>\n",
    "1. work (regular, chapters, colWorks, external_work)\n",
    "1. user\n",
    "1. collection\n",
    "1. series\n",
    "1. OTHER: search, tag, comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282d4da",
   "metadata": {},
   "source": [
    "cols = url, work_type, url_type, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db6bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import AO3\n",
    "\n",
    "url = \"https://archiveofourown.org/works/14392692/chapters/33236241\"\n",
    "workid = AO3.utils.workid_from_url(url)\n",
    "print(f\"Work ID: {workid}\")\n",
    "work = AO3.Work(workid)\n",
    "print(f\"Chapters: {work.title}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71485cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in newer urls\n",
    "with open(\"newRemain.json\") as infile:\n",
    "    newRemain = json.load(infile)\n",
    "\n",
    "# load in all urls\n",
    "temp = []\n",
    "with open(\"jsonOutput_12-27-22.json\") as infile:\n",
    "    data = json.load(infile)\n",
    "    for line in data:\n",
    "        if \"<Added:\" not in line:\n",
    "            temp.append(line)\n",
    "\n",
    "# define func to assign dates\n",
    "def ten(x):\n",
    "    if x in newRemain: return datetime(2022, 1, 11)\n",
    "    else: return datetime(2017, 5, 31)\n",
    "\n",
    "# load URLs into DF\n",
    "df1 = pd.DataFrame(temp)\n",
    "df1 = df1.rename(columns={0: \"url\"})\n",
    "\n",
    "# add date_added\n",
    "df1[\"date_added\"] = df1[\"url\"].apply(ten)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aac072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07fe36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get other database\n",
    "now = datetime.now()\n",
    "fileName = f\"others_{now.strftime('%m-%d-%y %H:%M:%S')}.csv\"\n",
    "\n",
    "dfOther = df1.loc[df1[\"url\"].str.contains(\"archiveofourown.org\") == False]\n",
    "dfOther.reset_index().drop(columns=[\"index\"])\n",
    "\n",
    "dfOther.to_csv(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.loc[df1[\"url\"].str.contains(\"archiveofourown.org\")]\n",
    "df1 = df1.reset_index().drop(columns=[\"index\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dbc825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add work_type, url_type, id\n",
    "def yu(x):\n",
    "    return getTypeAndId(x)[0]\n",
    "\n",
    "df1[\"url_type\"] = df1[\"url\"].apply(yu)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab58438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def typeToWorkType(x):\n",
    "    if x == \"users\": return \"user\"\n",
    "    elif x == \"collections\": return \"collection\"\n",
    "    elif x == \"search\": return \"search\"\n",
    "    elif x == \"tags\": return \"tag\"\n",
    "    elif x == \"series\": return \"series\"\n",
    "    elif x == \"works\": return \"work\"\n",
    "    elif x == \"chapters\": return \"work\"\n",
    "    elif x == \"external_works\": return \"work\"\n",
    "    elif 'collections:' in x: return \"work\"\n",
    "    elif x == \"comments\": return \"comments\"\n",
    "    else: return \"UNDEFINED\"\n",
    "    \n",
    "df1[\"work_type\"] = df1[\"url_type\"].apply(typeToWorkType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"all-urls.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ef3b3",
   "metadata": {},
   "source": [
    "* first_df1.csv - holds all non-duplicates AO3 urls as of 12/29/2022\n",
    "\n",
    "- **url:** url of fic\n",
    "- **date_added:** date url was imported from readinglist\n",
    "- **id:** id of work\n",
    "- **url_type:** \n",
    "- **work_type:** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8768ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"first_df1.csv\").drop_duplicates(\"url\").reset_index().drop(columns=[\"Unnamed: 0\",\"index\"])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99cc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make noDupUrls\n",
    "dfUrls = df1\n",
    "dfUrls = dfUrls.drop_duplicates([\"url\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c642d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by=['url_type',\"id\"], ascending=False).to_excel(\"tester.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19976c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tets.merge(dfUrls, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make noDupWorks\n",
    "dfWorks = df1\n",
    "dfWorks.drop_duplicates(subset=[\"\"]).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
