{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c63b0e5",
   "metadata": {},
   "source": [
    "# FFN Analysis\n",
    "- <b>Name:</b> Sofia Kobayashi\n",
    "- <b>Date:</b> 12/10/202\n",
    "- <b>Description:</b> Helper functions for ffv9.1 Jupyter Notebook\n",
    "\n",
    "#### <b><u>Functions Table of Contents</u></b>\n",
    "1. [Cleaning & Incoming Data functions](#sec1)\n",
    "1. [Display & Interface functions](#sec2)\n",
    "1. [Search functions](#sec3)\n",
    "1. [Meta-functions: testing & report](#sec4)\n",
    "1. [Fic functions](#sec5)\n",
    "\n",
    "1. [Misc. functions](#sec6)\n",
    "1. [Single-use functions](#sec7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8880637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT STATEMENTS & GLOBAL VARIABLES\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "known_work_types = [\"works\",\"collections\",\"series\",\"users\",\"tags\",\"search\",\"external_works\",\"comments\",\"chapters\"]\n",
    "masterNoDupUrls = \"MASTER_noDupURLs.json\"\n",
    "masterNoDupWorks = \"MASTER_noDupWorks.json\"\n",
    "masterOthers = \"MASTER_others.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd6f02",
   "metadata": {},
   "source": [
    "<a id=\"sec1\"></a>\n",
    "## Cleaning & Incoming Data Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c95b2",
   "metadata": {},
   "source": [
    "<u>Types of URLs</u>\n",
    "1. works (regular, chapters, colWorks\n",
    "1. external works\n",
    "1. users\n",
    "1. collections \n",
    "1. search\n",
    "1. tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cc1397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://archiveofourown.org/works/search?utf8=%E2%9C%93&commit=Search&work_search%5Bquery%5D=&work_search%5Btitle%5D=Assembly+of+Pain%2C+Happiness%2C+%26+Feelings.&work_search%5Bcreators%5D=&work_search%5Brevised_at%5D=&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bsingle_chapter%5D=0&work_search%5Bword_count%5D=&work_search%5Blanguage_id%5D=&work_search%5Bfandom_names%5D=&work_search%5Brating_ids%5D=&work_search%5Bcharacter_names%5D=TommyInnit+%28Video+Blogging+RPF%29&work_search%5Brelationship_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Bcomments_count%5D=&work_search%5Bbookmarks_count%5D=&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc#:~:text=Works%20List-,Assembly%20of%20Pain%2C%20Happiness%2C%20%26%20Feelings.,-by%20RandomlySane\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('search',\n",
       " 'works/search?utf8=%E2%9C%93&commit=Search&work_search%5Bquery%5D=&work_search%5Btitle%5D=Assembly+of+Pain%2C+Happiness%2C+%26+Feelings.&work_search%5Bcreators%5D=&work_search%5Brevised_at%5D=&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bsingle_chapter%5D=0&work_search%5Bword_count%5D=&work_search%5Blanguage_id%5D=&work_search%5Bfandom_names%5D=&work_search%5Brating_ids%5D=&work_search%5Bcharacter_names%5D=TommyInnit+%28Video+Blogging+RPF%29&work_search%5Brelationship_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Bcomments_count%5D=&work_search%5Bbookmarks_count%5D=&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc#:~:text=Works%20List-,Assembly%20of%20Pain%2C%20Happiness%2C%20%26%20Feelings.,-by%20RandomlySane')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing getting type & id from: \n",
    "url_0 = 'https://archiveofourown.org/collections/WorksOfGreatQualityAcrossTheFandoms/works/29387814'\n",
    "url_1 = 'https://archiveofourown.org/works/22269148/chapters/53178208'\n",
    "url_2 = 'https://archiveofourown.org/users/miscellea/pseuds/The%20Feels%20Whale'\n",
    "url_3 = 'https://archiveofourown.org/collections/TheCrackheadBible/works?commit=Sort+and+Filter&include_work_search%5Bfandom_ids%5D%5B%5D=3828398&page=6&utf8=%E2%9C%93&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Blanguage_id%5D=&work_search%5Bother_tag_names%5D=&work_search%5Bquery%5D=kudos%3A+%26gt%3B1000&work_search%5Bsort_column%5D=kudos_count&work_search%5Bwords_from%5D=25000&work_search%5Bwords_to%5D='\n",
    "url_4 = 'https://archiveofourown.org/external_works/637417'\n",
    "url_5 = 'https://archiveofourown.org/tags/Danny%20Phantom/works'\n",
    "url_6 = \"https://archiveofourown.org/collections/Clever_Crossovers_and_Fantastic_Fusions\"\n",
    "url_7 = \"https://archiveofourown.org/works?commit=Sort+and+Filter&work_search%5Bsort_column%5D=kudos_count&work_search%5Bother_tag_names%5D=&work_search%5Bexcluded_tag_names%5D=&work_search%5Bcrossover%5D=&work_search%5Bcomplete%5D=&work_search%5Bwords_from%5D=&work_search%5Bwords_to%5D=&work_search%5Bdate_from%5D=&work_search%5Bdate_to%5D=&work_search%5Bquery%5D=&work_search%5Blanguage_id%5D=&tag_id=L%C3%A1n+Q%C7%90r%C3%A9n*s*M%C3%A8ng+Y%C3%A1o+%7C+J%C4%ABn+Gu%C4%81ngy%C3%A1o\"\n",
    "url_8 = \"https://archiveofourown.org/works/search?utf8=%E2%9C%93&commit=Search&work_search%5Bquery%5D=&work_search%5Btitle%5D=Assembly+of+Pain%2C+Happiness%2C+%26+Feelings.&work_search%5Bcreators%5D=&work_search%5Brevised_at%5D=&work_search%5Bcomplete%5D=&work_search%5Bcrossover%5D=&work_search%5Bsingle_chapter%5D=0&work_search%5Bword_count%5D=&work_search%5Blanguage_id%5D=&work_search%5Bfandom_names%5D=&work_search%5Brating_ids%5D=&work_search%5Bcharacter_names%5D=TommyInnit+%28Video+Blogging+RPF%29&work_search%5Brelationship_names%5D=&work_search%5Bfreeform_names%5D=&work_search%5Bhits%5D=&work_search%5Bkudos_count%5D=&work_search%5Bcomments_count%5D=&work_search%5Bbookmarks_count%5D=&work_search%5Bsort_column%5D=_score&work_search%5Bsort_direction%5D=desc#:~:text=Works%20List-,Assembly%20of%20Pain%2C%20Happiness%2C%20%26%20Feelings.,-by%20RandomlySane\"\n",
    "url_9 = \"https://archiveofourown.org/chapters/747149?show_comments=true\"\n",
    "url_10 = \"https://archiveofourown.org/collections/asoiaftimetraveltransmigration/works/29620161\"\n",
    "url_11 = \"https://archiveofourown.org/bookmarks?commit=Sort+and+Filter&bookmark_search%5Bsort_column%5D=created_at&include_bookmark_search%5Brelationship_ids%5D%5B%5D=27817261&bookmark_search%5Bother_tag_names%5D=&bookmark_search%5Bother_bookmark_tag_names%5D=&bookmark_search%5Bexcluded_tag_names%5D=&bookmark_search%5Bexcluded_bookmark_tag_names%5D=&bookmark_search%5Bbookmarkable_query%5D=&bookmark_search%5Bbookmark_query%5D=&bookmark_search%5Blanguage_id%5D=&bookmark_search%5Brec%5D=0&bookmark_search%5Bwith_notes%5D=0&user_id=kyme\"\n",
    "url_12 = \"https://archiveofourown.org/collections:TheCrackheadBible/15774906\"\n",
    "\n",
    "import re\n",
    "def getTypeAndId(url):\n",
    "    \"\"\"Give an AO3 url. Returns a tuple with (type-of-work, work-id). Type = works, series, tags, etc.\n",
    "    If type = 'collections', assumed to be a colWork, returns (collections:colName, workId).\n",
    "    Depends on: re\"\"\"\n",
    "    # Check if it's a search result\n",
    "#     print(url)\n",
    "    if (\"works?\" in url) or (\"search?\" in url) or (\"bookmarks?\" in url):\n",
    "        pattern = re.compile('archiveofourown.org/(.+)')\n",
    "        search = pattern.findall(url)[0]\n",
    "        return(\"search\", search)\n",
    "    \n",
    "    if \"collections:\" in url:\n",
    "        pattern = re.compile (\"archiveofourown.org/collections:(.+)/(\\d+)\")\n",
    "        search = pattern.findall(url)[0]\n",
    "        url = f'https://archiveofourown.org/collections/{search[0]}/works/{search[1]}'\n",
    "    \n",
    "    # Find work type\n",
    "    pattern = re.compile(\"(archiveofourown.org/)(\\w+)/\")\n",
    "    info = pattern.findall(url)\n",
    "    wType = info[0][1]\n",
    "\n",
    "    # Check if it's an unknown type\n",
    "    if wType not in known_work_types:\n",
    "        raise Exception(f'WorkType not in global variable known_work_types!\\n- url: {url}\\n- output type: {wType}')\n",
    "    \n",
    "    # *** FIND TYPE & ID ***\n",
    "    # If work type is 'collections', I think it has to be a colWork (different URL format)\n",
    "    if wType == \"collections\":\n",
    "        pattern = re.compile(\"archiveofourown.org/collections/(\\w+)/works/(\\d+)\")\n",
    "        info2 = pattern.findall(url)\n",
    "        \n",
    "        # Check if it's a colWork\n",
    "        if info2 == []:\n",
    "            pattern = re.compile(\"archiveofourown.org/collections/(.+)$\")\n",
    "            info3 = pattern.findall(url)\n",
    "\n",
    "            # Check if it's NOT a colWork or collection\n",
    "            if info3 == []:\n",
    "                raise Exception(f'type=\"collections\", but not a colWork or collection!\\n- url: {url}\\n- output type: {wType}')\n",
    "             \n",
    "            #Return collection data\n",
    "            return (wType, info3[0])\n",
    "        \n",
    "        # Return colWork data\n",
    "        colName = info2[0][0]\n",
    "        wId = info2[0][1]\n",
    "        return(f\"{wType}:{colName}\", wId)\n",
    "\n",
    "    elif wType == \"users\":\n",
    "        pattern = re.compile(\"archiveofourown.org/users/(\\w+)\")\n",
    "        authorName = pattern.findall(url)[0]\n",
    "        return (wType, authorName)\n",
    "    \n",
    "    elif wType == \"tags\":\n",
    "        pattern = re.compile(\"archiveofourown.org/tags/(.+)/works\")\n",
    "        tag = pattern.findall(url)[0]\n",
    "        return (wType, tag)\n",
    "    \n",
    "    # Else, return type & idNum\n",
    "    else:\n",
    "        pattern = re.compile(\"(archiveofourown.org/)(\\w+)/(\\d+)\")\n",
    "        info = pattern.findall(url)\n",
    "        return (wType, info[0][2])\n",
    "\n",
    "# for i in range(12):\n",
    "#     print(getTypeAndId(url_12))\n",
    "\n",
    "# getTypeAndId(url_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a416bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def combineToTxt(dirPath):\n",
    "    \"\"\"\n",
    "    Takes a string-path to a directory full of TXT files. Function then combines all files into 1 TXT file,\n",
    "    will only add all text to 1 file, no de-duppinhg.\n",
    "    \"\"\"\n",
    "    # Get date\n",
    "    now = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    # Get all files in given directory\n",
    "    allFiles = get_all_files(dirPath)\n",
    "    others = []\n",
    "\n",
    "    # Get all lines in files\n",
    "    for file in allFiles:\n",
    "        # read in file\n",
    "        with open(f\"{dirPath}/{file}\", \"r\") as infile:\n",
    "            for line in infile:\n",
    "                line.strip() # remove trailing whitespace\n",
    "                others.append(line)\n",
    "\n",
    "    # Write to json\n",
    "    with open(f\"txtOutput_{now}.txt\",\"w\") as outfile:\n",
    "        outfile.writelines(others)\n",
    "    \n",
    "    return f\"txtOutput_{now}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38e05560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jsonOutput_12-27-22.json'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "def combineToJson(dirPath):\n",
    "    \"\"\"Takes a string-path to a directory full of TXT files. Function then combines all files into 1 JSON file.\"\"\"\n",
    "    # Get date\n",
    "    now = datetime.now().strftime(\"%m-%d-%y\")\n",
    "    \n",
    "    # Get all files in given directory\n",
    "    allFiles = get_all_files(dirPath)\n",
    "    others = []\n",
    "\n",
    "    # Get all lines in files\n",
    "    for file in allFiles:\n",
    "        # read in file\n",
    "        with open(f\"{dirPath}/{file}\", \"r\") as infile:\n",
    "            for line in infile:\n",
    "                line = line.strip() #to get rid of \\n  \n",
    "                others.append(line)\n",
    "\n",
    "    # Write to json\n",
    "    with open(f\"jsonOutput_{now}.json\",\"w\") as outfile:\n",
    "        json.dump(others, outfile)\n",
    "        \n",
    "    return f\"jsonOutput_{now}.json\"\n",
    "\n",
    "# combineToJson(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8b643be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def add_to_masterfiles(urlFile):\n",
    "    \"\"\"\n",
    "    Takes ONE txt file of URLs, appends new urls (probably from a new reading list) to the 3 MASTER json files:\n",
    "    MASTER_noDupURLs, MASTER_noDupWorks, MASTER_others. Pair with `combineToTxt(dirPath)` to convert whole \n",
    "    folders of TXT files.\n",
    "    Returns 'success' if successful. \n",
    "    \"\"\"\n",
    "    # Get current date & initialize 3 lists\n",
    "    now = datetime.now()\n",
    "    date_str = f\"<Added: {now.strftime('%m-%d-%y %H:%M:%S')}>\"\n",
    "    \n",
    "    \n",
    "    # Initialize variables\n",
    "    files = [\"MASTER_noDupURLs.json\", \"MASTER_noDupWorks.json\", \"MASTER_others.json\"]\n",
    "    \n",
    "    for file in files:\n",
    "        if not os.path.isfile(file):\n",
    "            with open(file,\"w\") as outfile:\n",
    "                json.dump([], outfile)\n",
    "            print(f\"Made {file}\")\n",
    "    \n",
    "    newNoDupURLs = []\n",
    "    newNoDupWorks = []\n",
    "    newOthers = []\n",
    "\n",
    "    \n",
    "    # Read in original files\n",
    "    with open(files[0], \"r\") as infile:\n",
    "        noDupURLs = json.load(infile)\n",
    "    \n",
    "    # rules a little different for noDupWorks bc it's formatted: [[typeI, url], ...\n",
    "    with open(files[1], \"r\") as infile:  \n",
    "        noDupWorks = json.load(infile)\n",
    "        if noDupWorks == []: typeIdList = []\n",
    "        else: \n",
    "            typeIdList = list(list(zip(*noDupWorks))[0])\n",
    "        \n",
    "    with open(files[2], \"r\") as infile:\n",
    "        others = json.load(infile)\n",
    "\n",
    "    totalLen = 0\n",
    "    # Read in new URLs \n",
    "    with open(urlFile, \"r\") as infile:\n",
    "        for line in infile:\n",
    "            line = line.strip() #to get rid of \\n\n",
    "#             print(line) #DID SOMETHING GO WRONG?\n",
    "            # if not an AO3 url\n",
    "            # 1. others filter\n",
    "            if \"archiveofourown.org\" not in line:\n",
    "                if line not in others:\n",
    "                    newOthers.append(line)\n",
    "\n",
    "            else:\n",
    "                # 2. noDupUrls filter\n",
    "                if line not in noDupURLs:\n",
    "                    newNoDupURLs.append(line)\n",
    "\n",
    "                # 3. noDupWorks filter\n",
    "                typeId = list(getTypeAndId(line))\n",
    "                if typeId not in typeIdList:\n",
    "                    typeIdList.append(typeId)\n",
    "                    pair = [typeId, url]\n",
    "                    newNoDupWorks.append(pair)\n",
    "            totalLen += 1\n",
    "    \n",
    "    # Format & Write newly added-to files\n",
    "    fileTypes = [[noDupURLs, newNoDupURLs, files[0]], \n",
    "                 [noDupWorks, newNoDupWorks, files[1]], \n",
    "                 [others, newOthers, files[2]]]\n",
    "        \n",
    "    for original, new, file in fileTypes:\n",
    "        original.append(date_str) # add date stamp\n",
    "        original.extend(new) # add new URLs\n",
    "        \n",
    "        # Write newly appended-lists\n",
    "        with open(file, \"w\") as infile:\n",
    "            json.dump(original, infile)\n",
    "        \n",
    "    # print addition report\n",
    "    print(f\"There were {totalLen} url(s) in '{urlFile}'\")\n",
    "    print(f\"Added {len(newNoDupURLs)} url(s) to MASTER_noDupURLs.json\")\n",
    "    print(f\"Added {len(newNoDupWorks)} url(s) to MASTER_noDupWorks.json\")\n",
    "    print(f\"Added {len(newOthers)} url(s) to MASTER_others.json\")\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "    \n",
    "# add_to_masterfiles(\"txtOutput_12-25-22.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "226e31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_to_masterfiles(dirPath):\n",
    "    \"\"\"\n",
    "    Takes a directory (full of URL TXT files), makes a combined TXT files, then adds all those URLs to MASTER\n",
    "    files.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    txtFile = combineToTxt(dirPath)\n",
    "    result = add_to_masterfiles(txtFile)\n",
    "    print(f\"{result.title()}!\")\n",
    "\n",
    "# all_to_masterfiles(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581f55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readinglist_to_masterfiles():\n",
    "    \"\"\"\n",
    "    Makes TXT file from Safari reading list, adds all those urls to master files.\n",
    "    Returns nothing.\n",
    "    \"\"\"\n",
    "    txtFile = getReadingList()\n",
    "    result = add_to_masterfiles(txtFile)\n",
    "    print(f\"{result.title()}!\")\n",
    "\n",
    "# readinglist_to_masterfiles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e267c5",
   "metadata": {},
   "source": [
    "<a id=\"sec2\"></a>\n",
    "## Display & Interface functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30653c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCorrectInput(allowedList):\n",
    "    \"\"\"Gets & returns user input but keeps prompting user until input is within correctList.\"\"\"\n",
    "    # run loop until input is within correctList\n",
    "    passes = False\n",
    "    while not passes: \n",
    "        passes = True\n",
    "        ans = input()\n",
    "        if ans not in allowedList:\n",
    "            passes = False\n",
    "    \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebf0d1",
   "metadata": {},
   "source": [
    "<a id=\"sec3\"></a>\n",
    "## Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7989095a",
   "metadata": {},
   "source": [
    "<a id=\"sec4\"></a>\n",
    "## Meta-functions: Testing & Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6fb7b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reportMasters():\n",
    "    for file in [masterNoDupUrls, masterNoDupWorks, masterOthers]:\n",
    "        with open(file) as infile:\n",
    "            data = json.load(infile)\n",
    "            print(f\"{file} has {len(data)} url(s)\")\n",
    "            \n",
    "# reportMasters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9585d46",
   "metadata": {},
   "source": [
    "<a id=\"sec5\"></a>\n",
    "## Fic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a997164c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08913ca8",
   "metadata": {},
   "source": [
    "<a id=\"sec6\"></a>\n",
    "## Misc. functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e9f0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "def get_all_files(dirName): \n",
    "    \"\"\"Takes a string - name of directory. Returns list of ALL files within that directory minus the .DS_Store\"\"\"\n",
    "    allFiles = [f for f in listdir(dirName)]\n",
    "    if \".DS_Store\" in allFiles:\n",
    "        allFiles.remove(\".DS_Store\")\n",
    "    return allFiles\n",
    "\n",
    "# get_all_files(\"urlsOutput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ff8f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReadingList():\n",
    "    \"\"\"extracturls.py ~ This script gets a list of all the URLs in Safari Reading List, and\n",
    "    writes them all to a file. Requires Python 3. ~ from someone on StackOverflow\"\"\"\n",
    "    #!/usr/bin/env python\n",
    "    import os\n",
    "    import plistlib\n",
    "\n",
    "    # Get current date \n",
    "    now = datetime.now()\n",
    "    current_date = now.strftime(\"%m-%d-%y\")\n",
    "\n",
    "    # set file paths\n",
    "    INPUT_FILE  = os.path.join(os.environ['HOME'], 'Library/Safari/Bookmarks.plist')\n",
    "    OUTPUT_FILE = f\"readinglist_{current_date}.txt\"\n",
    "\n",
    "    # Load and parse the Bookmarks file\n",
    "    with open(INPUT_FILE, 'rb') as plist_file:\n",
    "        plist = plistlib.load(plist_file)\n",
    "\n",
    "    # Look for the child node which contains the Reading List data.\n",
    "    # There should only be one Reading List item\n",
    "    children = plist['Children']\n",
    "    for child in children:\n",
    "        if child.get('Title', None) == 'com.apple.ReadingList':\n",
    "            reading_list = child\n",
    "\n",
    "    # Extract the bookmarks\n",
    "    bookmarks = reading_list['Children']\n",
    "\n",
    "    # For each bookmark in the bookmark list, grab the URL\n",
    "    urls = (bookmark['URLString'] for bookmark in bookmarks)\n",
    "\n",
    "    # Write the URLs to a file\n",
    "    with open(OUTPUT_FILE, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(urls))\n",
    "    \n",
    "    print(f\"Wrote to {OUTPUT_FILE}\")\n",
    "    return OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e670ea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOnlyAO3(fileName):\n",
    "    \"\"\"DEPRICATED - Takes a TXT file of URLs & returns 2 lists: [[the AO3 link], [non-AO3 links]]\"\"\"\n",
    "    # Read in file of URLs\n",
    "    with open(fileName, \"r\") as infile:\n",
    "        lines = infile.readlines()\n",
    "    \n",
    "    # Sort URLs into archive & non-archive lists \n",
    "    archive = []\n",
    "    notArchive = []\n",
    "    for line in lines:\n",
    "        if \"archiveofourown.org\" in line:\n",
    "            archive.append(line)\n",
    "        else:\n",
    "            notArchive.append(line)\n",
    "\n",
    "    return [archive, notArchive]\n",
    "    \n",
    "    \n",
    "#     # Write archive URLs to file\n",
    "#     fileNice = fileName.split(\"/\")[-1] \\\n",
    "#                         .replace(' ','-')\n",
    "#     archiveFile = f\"archive_{fileNice}\"\n",
    "#     with open(archiveFile, \"w\") as outfile:\n",
    "#         outfile.writelines(archive)\n",
    "#         print(f\"Wrote {len(archive)} AO3 link(s) to {archiveFile}\")\n",
    "    \n",
    "#     # Write non-archive URLs to file\n",
    "#     notArchiveFile = f\"notArchive_{fileNice}\"\n",
    "#     with open(notArchiveFile, \"w\") as outfile:\n",
    "#         outfile.writelines(notArchive)\n",
    "#         print(f\"Wrote {len(notArchive)} non-AO3 links to {notArchiveFile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c4281",
   "metadata": {},
   "source": [
    "<a id=\"sec7\"></a>\n",
    "## Single-use functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "042e297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCol():\n",
    "    \"\"\"Single use - Made Collection Works URLs from (idNum, collectionName) tuples.\"\"\"\n",
    "    # Reads in numColWorks TXT file\n",
    "    res = []\n",
    "    with open(\"urls/numColWorks.txt\", \"r\") as inFile:\n",
    "        lines = inFile.readlines()\n",
    "        \n",
    "    # Creates ColWorks URLs from the id & name\n",
    "    for line in lines:\n",
    "        data = line[:-1].split(\",\")\n",
    "        col = data[1]\n",
    "        idNum = data[0]\n",
    "        print(f\"https://archiveofourown.org/collections/{col}/works/{idNum}\")\n",
    "\n",
    "    # Writes ColWorks URLs\n",
    "    fileName = f\"urlsOutput/from_{'numColWorks'.lower().replace(' ','-')}.txt\"\n",
    "    with open(fileName, \"w\") as outFile:\n",
    "        print(f\"Writing to {fileName}\")\n",
    "        outFile.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4ba04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToStart(infile, strToAdd):\n",
    "    \"\"\"Single Use (kinda) - Takes a TXT file & string. Adds given string to the front of each line in the file, \n",
    "    writes a new file named 'from_{given file}'.\"\"\"\n",
    "    # Reads in given file\n",
    "    res = []\n",
    "    with open(f\"urls/urlFiles/{infile}\", \"r\") as inFile:\n",
    "        lines = inFile.readlines()\n",
    "        \n",
    "    # Attaches given string to front\n",
    "    for line in lines:\n",
    "        res.append(strToAdd+line)\n",
    "\n",
    "    # Overwrites given file with new \n",
    "    with open(f\"urlsOutput/from_{infile.lower().replace(' ','-')}\", \"w\") as outFile:\n",
    "        outFile.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d638220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786c3d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275e88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b33f56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
